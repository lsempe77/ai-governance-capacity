---
title: "Literature Review"
---

## Theoretical Foundations {#sec-literature}

### Implementation Science and Policy Capacity {#sec-lit-implementation}

The conceptual foundation of this study draws from implementation science, beginning with Pressman and Wildavsky's [-@pressman1973] observation that well-designed programs routinely fail in execution. The field addresses a fundamental question: what features distinguish policies that achieve implementation from those that remain aspirational?

Despite decades of implementation research, a puzzling gap persists: while scholars have identified multiple conditions for effective implementation, systematic measurement of these conditions across large policy samples remains rare. Most studies examine a handful of cases qualitatively, leaving uncertain whether identified implementation barriers are generalizable or context-specific. This study addresses that gap by operationalizing implementation capacity dimensions for systematic measurement across 2,100+ AI policies.

@mazmanian1983 identified six conditions for effective implementation: clear policy objectives, adequate causal theory, legal structuring, committed implementing officials, organized interest group support, and stable conditions. These conditions inform the capacity dimensions employed in this study: Clarity corresponds to their objectives condition, Resources captures their commitment requirements, Authority aligns with legal structuring, Accountability with monitoring, and Coherence with inter-agency coordination. However, the framework was developed for traditional policy domains where implementation processes are relatively stable. AI governance poses distinct challenges: rapid technological change makes "stable conditions" virtually impossible; private sector expertise concentration complicates "committed implementing officials"; and cross-border AI deployment undermines single-jurisdiction legal structuring.

@sabatier1986 synthesized top-down and bottom-up perspectives, arguing that both formal structure and implementing strategies matter. This framework motivates the focus on policy architecture rather than policy content alone. @lipsky1980 shifted focus to "street-level bureaucrats," the frontline workers whose discretionary decisions effectively *are* policy. For AI governance, this insight has concrete implications: even comprehensive legislation may fail if regulators lack expertise or mandate. Data protection authorities interpreting GDPR's algorithmic accountability provisions, or competition regulators assessing AI market power with limited technical staff, exemplify this challenge. The **Accountability (C4)** dimension captures whether policies constrain such discretion through monitoring and evaluation frameworks.

@grindle1996 identified four capacity types relevant to AI governance:

| Capacity Type | Our Dimension | Indicators |
|:---|:---|:---|
| Technical | C2 Resources | Expertise, training, technology |
| Administrative | C3 Authority | Legal mandate, organizational structure |
| Political | C5 Coherence | Cross-ministry coordination |
| Fiscal | C2 Resources | Budget allocation |

: State capacity mapping {#tbl-capacity-mapping}

These typologies, however, assume capacity types are empirically distinguishable. Grindle's framework treats technical, administrative, political, and fiscal capacity as separate constructs, yet AI governance may require their simultaneous deployment: technical expertise without legal authority achieves little, while legal mandate without fiscal resources remains hollow. The correlation structure among capacity dimensions thus becomes an empirical question—one this study addresses by scoring each dimension independently and examining their covariation patterns.

@fukuyama2013 argued that governance quality is conceptually distinct from democracy or GDP, proposing measurement through government outputs. This study adopts a similar approach: measuring institutional readiness through policy quality rather than inputs such as national wealth. However, Fukuyama's output-based measurement confronts a circularity problem: governance quality affects policy outputs, but poor outputs may reflect implementation failures rather than design weaknesses. This study addresses the circularity by measuring *ex ante* institutional arrangements (designated agencies, allocated budgets, articulated procedures) rather than *ex post* implementation success.

@andrews2017 introduced "building state capability" through iterative adaptation, warning against imposing "best practice" from high-income countries. The empirical findings support this perspective: developing countries that score well on capacity do so through different pathways than wealthy nations. Andrews' critique, however, raises measurement challenges: if capacity pathways are context-dependent, can universal scoring rubrics meaningfully compare across jurisdictions? This study navigates this tension by scoring *presence* of capacity features (budget allocation, institutional designation, monitoring systems) without prescribing *how* those features should be structured.

### The Capacity Gap in Digital Governance

Recent research documents persistent implementation gaps in digital regulation. @yeung2018 shows algorithmic regulation demands technical expertise most governments lack. @katzenbach2019 demonstrates that platform governance creates enforcement challenges traditional regulators struggle to address. Yet these studies examine high-income jurisdictions (primarily EU member states and the United States), leaving unclear whether capacity deficits are universal or concentrated in resource-constrained settings. Moreover, they diagnose capacity gaps qualitatively without measuring their magnitude or comparing across policy instruments.

@dafoe2018 argues that AI governance faces unique capacity challenges: rapid technological change outpacing regulatory adaptation, concentrated expertise in private sector rather than government, and international coordination problems where no single jurisdiction can effectively regulate global AI systems. This analysis is conceptually compelling but empirically underspecified: *which* governments lack capacity, *how much* capacity they lack, and *whether* capacity gaps are widening or narrowing remain unanswered. The argument also risks technological determinism—assuming AI's complexity inherently exceeds governmental capacity—without testing whether institutional design choices might compensate for technical complexity.

### Measurement Challenges and Contribution

Despite theoretical progress, **systematic capacity measurement remains rare**. Existing studies rely on qualitative assessments [@cihon2021] or expert surveys [@dafoe2020] that cover a handful of cases at most. This methodological limitation is not incidental: capacity assessment requires deep engagement with policy text—identifying institutional designations, interpreting budget commitments, tracing coordination mechanisms—which historically required human expertise and thus did not scale beyond small samples.

The resulting gap between theory and evidence is substantial. Implementation science offers sophisticated frameworks for understanding capacity requirements, yet lacks empirical evidence on capacity distributions across jurisdictions, policy types, or time periods. AI governance scholarship diagnoses capacity deficits but cannot quantify their severity or variation. Development economics measures state capacity through proxies (tax collection, bureaucratic quality indices) that may not capture domain-specific governance dimensions.

The LLM-based scoring approach developed in this study attempts to bridge that gap, enabling assessment across 2,100+ policies without sacrificing interpretive depth. However, automated scoring introduces validity concerns: can language models reliably interpret nuanced institutional arrangements? The inter-rater reliability analysis (ICC = 0.827) addresses this concern empirically, demonstrating that LLM ensemble scoring achieves agreement levels comparable to human expert coding.

**Contribution.** This study addresses three gaps in the literature. **First**, while implementation science has identified capacity conditions conceptually, systematic measurement across large policy samples remains absent. This study operationalizes five capacity dimensions and applies them to 2,100+ policies, enabling distributional analysis impossible with qualitative methods. **Second**, AI governance scholarship diagnoses capacity deficits but lacks empirical evidence on their magnitude, variation, or determinants. This study quantifies capacity levels across 70+ jurisdictions, tests whether national wealth predicts capacity, and examines whether capacity varies by policy type or jurisdiction income. **Third**, state capacity measurement typically relies on generic proxies (bureaucratic quality, regulatory quality indices) that may not capture domain-specific governance requirements. This study develops AI-governance-specific capacity indicators grounded in implementation science theory.

The findings challenge conventional wisdom: national wealth shows minimal association with implementation capacity once text quality is controlled. This contradicts the assumption, implicit in much AI governance discourse, that developing countries systematically lag in governance sophistication. As the following sections demonstrate, capacity deficits are nearly universal—high-income countries perform only marginally better than developing countries, and both groups exhibit similar within-group variation.
