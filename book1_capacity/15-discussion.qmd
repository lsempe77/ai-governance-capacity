---
title: "Discussion"
---

## Implications for Capacity Building {#sec-discussion}

Four patterns emerge from the empirical analysis with implications for how we understand AI governance capacity and how interventions might be designed to strengthen it. This section discusses each in turn, connects them to the theoretical foundations reviewed in @sec-literature, and identifies the limitations that qualify these conclusions.

### The Universal Capacity Deficit and the Limited Role of Wealth

Approximately 96.5% of AI policies worldwide score below 2.0/4.0 on implementation readiness. This capacity deficit proves universal, affecting the United States, European Union, and China comparably to developing countries. The finding challenges narratives that frame implementation weakness as a developing-country problem; it is, instead, a structural feature of AI governance globally.

Weakness concentrates in two dimensions. **Accountability (C4)** averages just 0.48/4.0—the weakest dimension in the entire framework. Governments are more than twice as likely to specify coordination mechanisms (C5: 1.07) as to establish monitoring and evaluation frameworks. This pattern likely reflects a political economy logic: accountability mechanisms create political risks by enabling external assessment of implementation failures. **Resources (C2)** averages 0.68/4.0. Policies routinely omit budget specifications, staffing plans, and technical infrastructure requirements. This omission may be strategic rather than accidental: committing specific resources constrains future budgetary discretion, while vague resource language preserves flexibility. These findings are consistent with @mazmanian1983's conditions for effective implementation—clear objectives, adequate resources, legal authority, and monitoring—which were identified precisely because they are routinely absent from policy design. What this study adds is the empirical demonstration that these absences are not sporadic but systematic.

Critically, GDP explains only 3.5% of country-level capacity variation. Rwanda (2.30–3.10× GDP predictions), Kenya, Brazil, and Uganda have all achieved sophisticated governance capacity despite modest GDPs ($800–$9,000). This aligns with @fukuyama2013's argument that governance quality is conceptually distinct from economic development, and with @andrews2017's critique of imposing "best practice" models: the overperforming developing countries did not transplant European or North American frameworks but built capacity through context-specific institutional choices. Brazil's strength derives from its data protection infrastructure and participatory governance tradition; Rwanda's from its centralised ICT-driven development strategy; Kenya's from its constitutional rights framework and vibrant civil society sector. Capacity emerges from political choices, not fiscal abundance.

### The Accountability Paradox and the Documentation Confound {#sec-disc-accountability}

The finding that Accountability (C4) is the weakest dimension globally—and the dimension with the *smallest* income-group gap (d = 0.15)—warrants dedicated attention. Accountability mechanisms (monitoring, evaluation, reporting, independent oversight) are precisely the features that implementation science identifies as critical for closing the gap between policy commitments and actual outcomes [@lipsky1980; @pressman1973]. Yet they are the features that governments most systematically omit.

This creates what might be termed the *accountability paradox*: the governance feature most needed to ensure implementation is the one least likely to be included in policy design. The paradox has a straightforward political explanation—accountability mechanisms create transparency that exposes implementation failures—but it has no straightforward policy solution. External mandates (e.g., international frameworks requiring periodic reporting) may partially address the problem, as countries responding to UNESCO or OECD peer review pressures might be more likely to establish monitoring systems than those designing policies in isolation. The efficiency frontier analysis (@fig-cap-frontier) provides suggestive evidence: overperforming countries like Brazil and Rwanda include stronger accountability provisions than underperformers at similar GDP levels, though the direction of causality remains uncertain.

**Documentation as confound and peer learning.** The income gap (d = 0.30) vanishes for well-documented policies (d = 0.04). Any study using policy text as data—whether for content analysis, topic modelling, or automated scoring—faces the risk that variation in documentation quality masquerades as variation in governance quality. The apparent North–South divide in AI governance may be, in substantial part, a documentation divide. For international organisations maintaining policy repositories, investing in comprehensive documentation of developing-country policies would improve the evidentiary basis for comparative governance research as much as any new analytical technique. The finding also raises an uncomfortable possibility: if the "governance gap" partly reflects documentation practices, decades of comparative policy research may have systematically overstated North–South differences across multiple policy domains.

Capacity also diffuses horizontally within income groups rather than cascading from wealthy countries. This finding contradicts the "Brussels Effect" [@bradford2020] as applied to AI governance and instead supports the policy learning literature's emphasis on peer-to-peer transfer [@simmons2006]. Brazil, India, and China prove central to developing-country policy networks, serving as regional reference points rather than conduits for wealthy-country frameworks.

South-South exchanges, regional capacity hubs, and peer review mechanisms may prove more effective than traditional North-South technical assistance models. Concrete mechanisms could include regional AI governance forums (building on existing structures like the African Union's AI strategy or Latin America's Red Iberoamericana), paired technical assistance between frontier developing countries and late adopters, and open-source governance tools adapted to developing-country institutional contexts.

### Limitations

Several limitations qualify these findings. **First**, the study measures policy *text* rather than policy *outcomes*. A policy scoring 4/4 on Accountability may nonetheless fail in implementation if the designated monitoring bodies lack capacity, political will, or independence. Text-based analysis captures design quality, not implementation effectiveness—a distinction that @lipsky1980 and @pressman1973 would emphasise. **Second**, the LLM scoring approach, while achieving excellent inter-rater reliability (ICC = 0.827), may share systematic biases across all three models. The models were trained on overlapping data that likely includes prominent AI governance documents, potentially inflating scores for policies resembling those in the training data. **Third**, the OECD.AI corpus reflects the Observatory's own coverage decisions: which countries are included, which policies are catalogued, and how thoroughly each is described. Countries with closer OECD ties may be overrepresented, and policies from non-OECD countries may be less comprehensively documented. **Fourth**, the study cannot establish causal relationships. The association between binding regulation and higher capacity scores, for instance, may reflect reverse causation: countries with stronger institutional capacity may be more likely to adopt binding legislation. **Fifth**, the 77%/18% split between high-income and developing countries limits statistical power for detecting within-developing-country patterns. Subgroup analyses (e.g., comparing Sub-Saharan Africa to Latin America) rely on small samples that reduce confidence in regional findings.
