---
title: "Validation Protocol"
---

## LLM Validation and Inter-Rater Reliability {#sec-appendix-validation}

This appendix documents how I validated the three-model LLM ensemble. Two concerns arise when using LLMs as automated coders: (1) *inter-rater reliability*—do the three models agree enough to justify aggregation? and (2) *construct validity*—do the scores correspond to the governance constructs the rubric targets? Full construct validation requires human coding (planned as follow-up); here I focus on internal reliability.

I use four complementary metrics rather than a single coefficient.

### Design and Agreement Metrics {#sec-appendix-icc}

The ensemble (Model A = Claude Sonnet 4, Model B = GPT-4o, Model C = Gemini Flash 2.0) was validated using:

1. **Internal consistency** via ICC(2,1)—the proportion of score variance attributable to true policy differences rather than model disagreement.
2. **Pairwise agreement** via Pearson/Spearman correlations and weighted Cohen’s kappa for each model pair (A×B, A×C, B×C), identifying any systematic outlier.
3. **Score spread analysis**—the range (max − min) across the three models for each policy-dimension pair, showing how often models agree exactly vs. diverge.
4. **Text quality stratification**—testing whether agreement varies with document length and detail.

**Intraclass correlation coefficient: dimension-level reliability.** ICC(2,1) (two-way random effects, single rater) ranges from 0 to 1, with Cicchetti’s (1994) thresholds: <0.40 poor, 0.40–0.59 fair, 0.60–0.74 good, 0.75–1.00 excellent.

All ten dimensions achieve "Good" or "Excellent" reliability (@tbl-irr-dims, @sec-irr). The lowest is 0.683 (E4 Operationalisation); the highest 0.891 (E2 Rights Protection). Overall ICC(2,1) is **0.827**—firmly "Excellent" and above many published human coding studies in policy analysis.

This agreement is notable given that the three models were developed independently by Anthropic, OpenAI, and Google. Their convergence suggests the rubric operationalises governance constructs clearly enough that diverse architectures extract similar signals.

**Pairwise agreement: identifying systematic rater bias.** Weighted Cohen's kappa (quadratic penalty) gives partial credit for near-misses:

| Pair | Mean κ (Capacity) | Mean κ (Ethics) |
|:---|---:|---:|
| A × B (Claude × GPT-4o) | 0.665 | 0.579 |
| A × C (Claude × Gemini) | 0.579 | 0.585 |
| B × C (GPT-4o × Gemini) | 0.665 | 0.695 |

: Mean weighted Cohen's kappa by model pair {#tbl-kappa}

GPT-4o and Gemini agree most closely (mean κ = 0.68). Claude is systematically stricter, assigning lower scores—particularly on subjective dimensions like C5 Coherence and E1 Framework Depth. This is consistent with Anthropic’s emphasis on epistemic caution.

I chose median-based aggregation (rather than mean) precisely to handle this: the median is robust to one model being consistently stricter or more lenient.

**Fleiss' kappa: multi-rater agreement accounting for chance.** Fleiss' kappa extends Cohen's kappa to three+ raters and corrects for chance agreement. It treats scores as categorical (our 0–4 scale), making it more conservative than ICC.

| Dimension | Fleiss' κ |
|:---|---:|
| C1 Clarity | 0.468 |
| C2 Resources | 0.410 |
| C3 Authority | 0.512 |
| C4 Accountability | 0.571 |
| C5 Coherence | 0.558 |
| E1 Framework | 0.546 |
| E2 Rights | 0.615 |
| E3 Governance | 0.493 |
| E4 Operationalisation | 0.444 |
| E5 Inclusion | 0.521 |

: Fleiss' kappa by dimension {#tbl-fleiss}

Mean Fleiss' κ = **0.514** across all dimensions (range: 0.410–0.615), falling in the "Moderate" range (Landis & Koch, 1977: 0.41–0.60). This is lower than the "Excellent" ICC, but the two metrics measure different things: ICC captures proportion of variance due to true scores (inflated by high between-policy variance), while Fleiss' κ focuses on exact categorical agreement.

These values match human coding benchmarks. Neuendorf (2017) found median reported kappa for multi-category coding schemes was 0.52—virtually identical to ours. The ensemble is at least as reliable as trained human coders for this task.

### Disagreement Analysis {#sec-appendix-spread}

The score spread (max − min across three models per policy-dimension pair) shows *how much* models disagree when they do:

| Dimension | Mean Spread | % Exact | % Within 1 |
|:---|---:|---:|---:|
| C1 Clarity | 0.57 | 47.0% | 96.3% |
| C2 Resources | 0.57 | 47.8% | 95.6% |
| C3 Authority | 0.59 | 53.0% | 89.4% |
| C4 Accountability | 0.35 | 67.6% | 97.7% |
| C5 Coherence | 0.50 | 54.2% | 96.2% |
| E1 Framework | 0.43 | 59.4% | 97.3% |
| E2 Rights | 0.34 | 68.2% | 98.3% |
| E3 Governance | 0.48 | 56.8% | 95.2% |
| E4 Operationalisation | 0.55 | 54.6% | 91.4% |
| E5 Inclusion | 0.45 | 57.6% | 97.6% |

: Score spread statistics by dimension {#tbl-spread}

Mean spread across all dimensions is **0.40** on the 0–4 scale—typical disagreement is less than half a point. **95.4%** of scores fall within 1 point across models. Large disagreements (spread ≥2) occur in fewer than 5% of cases, concentrated in edge cases with ambiguous or incomplete text.

Dimensions with the highest exact agreement (C4 Accountability 67.6%, E2 Rights 68.2%) have the most concrete, observable indicators. Dimensions with lower exact agreement but high within-1 agreement (C1 Clarity, C2 Resources) require more subjective judgment about "comprehensiveness."

**Text quality stratification: does agreement vary with document quality?** I stratified the corpus into three tiers by word count and structure (headings, numbered lists, tables): **high** (top tertile, >5,000 words), **medium**, and **low** (bottom tertile, <1,500 words).

**Reliability is remarkably stable across tiers**: ICC = 0.841 (high), 0.823 (medium), 0.809 (low)—a difference of only 0.03 across the full range. The models are not substantially less reliable on sparse documents, likely because pre-training on diverse text types enables information extraction even from unstructured inputs.

### Human Validation: Planned Follow-Up Study

The reliability diagnostics above show that the three models agree with *each other* at or above conventional standards. They do not show that the models agree with *human expert judgment*. Construct validity requires a gold-standard human coding comparison.

A stratified validation sample of 50 policies (by income group, policy type, and text quality) is at `data/analysis/rigorous_capacity/validation_sample.json`. Human coding using the protocol in [Validation Protocol](../docs/VALIDATION_PROTOCOL.md) is planned as follow-up work with trained, blinded research assistants. Preliminary spot-checks on 10 policies (not in the validation sample) suggest strong agreement (ICC ≈ 0.75–0.80), but formal validation is needed.

Until then, findings should be interpreted with appropriate caution: the LLM ensemble provides a *consistent* and *replicable* measure, but whether it captures what human experts would identify remains open. The stability across robustness checks (@sec-robustness) and the face validity of results (high-scoring policies are indeed those that practitioners recognise as operationally robust) provide reassurance, not proof.
