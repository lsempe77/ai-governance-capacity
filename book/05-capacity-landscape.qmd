---
title: "Capacity Landscape"
---

## The Global Landscape of AI Governance Capacity {#sec-cap-landscape}

::: {.callout-note appearance="simple"}
**Chapter summary.** This chapter presents the descriptive landscape of AI governance capacity across 2,216 policies and 70+ jurisdictions. We examine score distributions, income-group comparisons, regional patterns, policy-type variation, and country rankings.
:::

### Overall Score Distribution {#sec-cap-distribution}

Before examining differences across countries and policy types, we first establish the baseline landscape: what does AI governance capacity look like globally when all 2,216 policies are considered together? This aggregate view reveals not only central tendencies but also the distributional features that shape subsequent analyses. Understanding the overall landscape proves essential for contextualizing the gaps and clusters we identify in later sections.

![Distribution of capacity dimension scores across 2,216 policies. All five dimensions exhibit strong right skew with floor effects at zero.](../data/analysis/paper1_capacity/fig_distributions.png){#fig-cap-distributions}

The capacity composite score averages **0.83/4.00** (SD = 0.77) across all 2,216 policies — positioning the typical AI governance document substantially below the scale midpoint. @fig-cap-distributions reveals that this modest mean conceals considerable heterogeneity, with all five dimensions exhibiting pronounced right skew: most policies cluster at or near zero, while a smaller set of comprehensive policies extends into higher score ranges. The dimension-level means range from a low of **0.48** (C4 Accountability) to a high of **1.07** (C5 Coherence):

| Dimension | Mean | SD | Median |
|:---|---:|---:|---:|
| C1 Clarity & Specificity | 0.94 | 0.97 | 1.00 |
| C2 Resources & Budget | 0.68 | 0.89 | 0.00 |
| C3 Authority & Enforcement | 1.04 | 1.08 | 1.00 |
| C4 Accountability & M&E | 0.48 | 0.72 | 0.00 |
| C5 Coherence & Coordination | 1.07 | 0.97 | 1.00 |
| **Capacity composite** | **0.83** | **0.77** | **0.60** |

: Capacity dimension descriptive statistics {#tbl-cap-descriptives}

@tbl-cap-descriptives exposes a striking pattern in the architecture of AI governance globally. **Accountability (C4) is the weakest dimension** across all policies, with a mean of just 0.48 — less than half the strength of Coherence (C5) at 1.07. This gap reflects a systematic prioritization: policies are more than twice as likely to specify coordination mechanisms or clarify objectives as to establish monitoring and evaluation frameworks. Governments appear more comfortable articulating what they intend to do (Clarity) and how agencies should work together (Coherence) than committing to transparent oversight mechanisms that would enable external assessment of implementation progress.

The substantial standard deviations — ranging from 0.72 to 1.08 — indicate enormous within-dimension variation. A policy scoring 1.0 on Resources might have zero budget allocation or substantial dedicated funding; the heterogeneity spans the full range of implementation readiness. Perhaps most consequentially, **27.6% of all policies score exactly zero** on the capacity composite, indicating no discernible implementation infrastructure in the policy text. More than a quarter of documents in the OECD.AI Observatory function as announcements or aspirational statements rather than operational governance instruments. This floor effect — visible in the median values of 0.00 for Resources and Accountability — motivates the Tobit regression approach we employ in @sec-cap-determinants to correct for censoring bias.

### Income-Group Comparisons {#sec-cap-income}

The conventional narrative about AI governance assumes a clear North–South divide, with wealthy countries possessing sophisticated regulatory frameworks and developing countries struggling to match this capacity. Our data provide the first systematic test of this assumption across thousands of policies. The results prove more nuanced than this binary narrative suggests: while a gap exists in the raw data, its magnitude and robustness merit careful examination.

![Violin plots comparing capacity score distributions between high-income and developing countries. The overlap between distributions is substantial.](../data/analysis/paper1_capacity/fig_income_violins.png){#fig-cap-income-violins}

@fig-cap-income-violins reveals both the gap and its limitations visually. High-income countries score significantly higher on capacity (mean 0.87, SD 0.77) than developing countries (mean 0.65, SD 0.72), a difference that proves statistically significant by conventional standards:

| Metric | Value |
|:---|:---|
| HI mean (N = 1,700) | 0.87 |
| Developing mean (N = 397) | 0.65 |
| Welch's $t$ | 5.47 |
| $p$-value | < .001 |
| Cohen's $d$ | **0.30** |
| Mann-Whitney $U$ | 395,388 |

: Income-group capacity comparison {#tbl-cap-income}

@tbl-cap-income presents the conventional statistical evidence for an income-based capacity gap. The Welch's t-test yields t = 5.47, p < .001, providing strong statistical evidence against the null hypothesis of equal means. However, statistical significance at large sample sizes (N = 2,097) does not automatically imply substantive importance. The Cohen's d effect size of **0.30** falls into the "small" range by conventional standards, indicating that the distributions overlap considerably. Indeed, the violin plots show that many developing-country policies score above the high-income median, while many high-income policies cluster near zero.

Crucially — and foreshadowing findings detailed in @sec-robustness — this gap **vanishes entirely** when analyses are restricted to well-documented policies with good text quality (d = 0.04, n.s.). This sensitivity to text extraction quality raises important interpretive questions: Does the observed gap reflect genuine capacity differences, or does it emerge from systematic differences in how countries document their policies? Do developing countries produce shorter policy documents because they have less capacity to document, or because their policy dissemination practices differ? This measurement challenge proves central to understanding what the capacity gap means.

#### Dimension-Level Gaps

The aggregate capacity gap masks considerable heterogeneity across the five implementation dimensions. If the North–South divide reflected fundamental differences in state capacity, we would expect relatively uniform gaps across all dimensions. Instead, the pattern proves more differentiated, suggesting that specific capacity constraints — rather than generalized institutional weakness — drive observed differences.

![Boxplots of capacity scores by income group across all five dimensions.](../data/analysis/paper1_capacity/fig_income_boxplot.png){#fig-cap-income-boxplot}

@fig-cap-income-boxplot visualizes how the income gap varies across dimensions, with the gap most pronounced for Resources and Coherence, and smallest for Accountability. The dimension-level statistics reveal this pattern precisely:

| Dimension | HI Mean | Dev Mean | Diff | $d$ | $p$ |
|:---|---:|---:|---:|---:|:---|
| C1 Clarity | 0.98 | 0.74 | 0.24 | 0.30 | < .001 |
| C2 Resources | 0.70 | 0.43 | 0.27 | **0.32** | < .001 |
| C3 Authority | 1.09 | 0.86 | 0.23 | 0.23 | < .001 |
| C4 Accountability | 0.48 | 0.37 | 0.10 | **0.15** | .005 |
| C5 Coherence | 1.13 | 0.86 | 0.27 | 0.29 | < .001 |

: Dimension-level income gaps {#tbl-cap-dim-gaps}

@tbl-cap-dim-gaps confirms hypothesis H3: the capacity gap proves largest in **Resources (C2)** (d = 0.32) and smallest in **Accountability (C4)** (d = 0.15). This pattern makes intuitive sense when considered through the lens of implementation science. Specifying budgets, staffing plans, and technical infrastructure requires fiscal resources that correlate with national wealth — wealthy countries can commit larger absolute budgets and possess deeper pools of technical expertise to deploy. By contrast, designing monitoring and evaluation frameworks represents primarily a policy design choice rather than a resource constraint — developing countries can establish reporting requirements, evaluation mandates, and oversight bodies with minimal fiscal commitment.

The modest gap on Accountability carries an ironic implication: developing countries could relatively easily narrow the capacity divide by strengthening their weakest dimension, yet both income groups underperform on accountability mechanisms. The universally low C4 scores (0.48 for high-income, 0.37 for developing) suggest that the reluctance to establish transparent oversight mechanisms transcends wealth differences. Accountability frameworks create political risks by enabling external assessment of implementation failures — a concern that affects governments regardless of income level.

### Regional Patterns {#sec-cap-regional}

Income-group comparisons, while revealing aggregate patterns, risk obscuring important geographic heterogeneity within income categories. The developing-country category encompasses Latin American middle-income countries with sophisticated regulatory traditions, South Asian nations with large technology sectors but limited governance infrastructure, and Sub-Saharan African countries with nascent AI policy ecosystems. Similarly, the high-income group conflates North American and European regulatory leaders with smaller high-income countries that have produced minimal AI governance activity. Regional analysis provides finer-grained insight into governance patterns that income alone cannot capture.

![Heatmap of mean capacity scores by region and dimension. North America and Europe & Central Asia lead; Sub-Saharan Africa and South Asia trail.](../data/analysis/paper1_capacity/fig_region_heatmap.png){#fig-cap-region-heatmap}

@fig-cap-region-heatmap reveals that regional variation is substantial but not straightforwardly reducible to income differences. Several patterns challenge simplistic assumptions about governance capacity. **North America (NAM)** leads across all five dimensions, driven primarily by the United States and Canada's extensive policy portfolios and consistently high-scoring individual documents. The North American strength appears most pronounced on Resources (C2) and Accountability (C4) — dimensions where fiscal capacity and regulatory tradition matter most.

**Europe & Central Asia (ECA)** shows the broadest dimensional coverage, with particular strength in Coherence (C5) reflecting the European Union's multilevel coordination mechanisms. The EU's extensive cross-border governance infrastructure — including the AI Act, Digital Services Act, and GDPR — establishes coordination frameworks that national policies reference and build upon. This regional coordination advantage distinguishes Europe from other high-income regions where governance remains more fragmented across national boundaries.

Perhaps most striking are the developing regions that exceed income-predicted performance. **Latin America & Caribbean (LAC)** scores above its income-group average across multiple dimensions, with particular strength in Authority (C3). Countries like Brazil, Colombia, and Argentina have adopted binding AI legislation with clear enforcement mechanisms, demonstrating that regulatory sophistication does not require first-world wealth. **Sub-Saharan Africa (SSA)**, while scoring lowest overall, shows surprising strength in Authority — several African countries (Kenya, Rwanda, South Africa) have enacted AI-specific legislation with legal mandates and compliance mechanisms that exceed what many wealthy countries have adopted.

These regional patterns suggest that governance capacity reflects institutional and political factors beyond simple wealth accumulation. Regulatory traditions, regional coordination frameworks, and policy diffusion networks shape capacity in ways that income alone cannot explain. We return to these themes in @sec-cap-dynamics, where we examine how policies spread across countries and regions.

### Policy-Type Variation {#sec-cap-policy-type}

Not all policy documents serve the same function or possess the same implementation obligations. National AI strategies articulate long-term visions and coordinate across sectors but may lack enforcement teeth. Binding regulations establish legal requirements with compliance mechanisms but may sacrifice flexibility. Ethics guidelines provide normative frameworks without operational detail. These functional differences suggest that capacity scores should vary systematically by policy type — not because some jurisdictions lack capacity but because different document types serve different governance purposes.

![Capacity scores by policy type. Binding regulations score highest; guidelines and principles score lowest.](../data/analysis/paper1_capacity/fig_policy_type.png){#fig-cap-policy-type}

@fig-cap-policy-type confirms this intuition clearly. **Binding regulation** — including laws, executive orders, and regulatory frameworks — scores highest across all dimensions, with particularly pronounced advantages on Authority (C3) and Accountability (C4). This makes structural sense: legal instruments must specify enforcement mechanisms, define responsible agencies, and establish monitoring procedures to be judicially enforceable. The high Authority scores reflect the legal mandates that binding regulations inherently possess, while elevated Accountability scores indicate that laws more frequently establish reporting requirements and evaluation frameworks than softer policy instruments.

**National strategies** occupy the middle ground, scoring moderately across dimensions with relative strength in Clarity (C1) and Coherence (C5). Strategic documents excel at articulating clear objectives and coordinating across government agencies — their primary intended function — while naturally scoring lower on Resources and Authority, which strategies often leave to subsequent implementing legislation. This pattern validates our measurement approach: scores reflect document content appropriate to policy type rather than applying a one-size-fits-all standard.

**Guidelines and principles** score lowest, reflecting their aspirational rather than operational character. Ethics guidelines typically enumerate desirable AI properties (transparency, fairness, accountability) without specifying who must implement these principles, how compliance will be monitored, or what resources will be allocated. These documents serve important functions — establishing normative frameworks, coordinating international principles, guiding voluntary adoption — but they deliberately avoid the operational specificity that would generate high capacity scores. The low scores on guidelines thus reflect accurate measurement of their non-binding character rather than a failure of governance.

This policy-type variation carries important implications for cross-national comparison. Jurisdictions that rely primarily on voluntary guidelines will systematically score lower than those adopting binding legislation, even if the voluntary approach proves effective through industry self-regulation. Our subsequent analyses control for policy type to ensure that apparent capacity differences reflect genuine institutional variation rather than strategic choices about governance instruments.

### Country Rankings {#sec-cap-rankings}

Aggregate statistics and distributional analyses provide essential context, but policymakers and researchers often want direct answers to a simpler question: which countries demonstrate the strongest AI governance capacity, and what distinguishes leaders from laggards? Country-level rankings compress our multidimensional framework into a single evaluative scale, inevitably losing nuance but gaining interpretive clarity. These rankings reflect both the number of policies each jurisdiction has produced and the average quality of those policies — a country with five excellent policies will rank higher than one with ten mediocre ones.

![Temporal trends in capacity scores across the 2017–2025 period.](../data/analysis/paper1_capacity/fig_temporal_trend.png){#fig-cap-temporal}

@fig-cap-temporal shows the evolution of capacity scores over time, revealing that aggregate capacity has remained relatively stable since 2019 despite the proliferation of policies. This stability suggests that policy quantity has not translated into quality improvement — the rapid expansion of AI governance activity has produced many low-scoring documents alongside continued high-quality policy development by leading jurisdictions.

The top-scoring jurisdictions combine large policy portfolios with consistently high-quality individual policies, demonstrating sustained commitment to implementation-ready governance rather than one-off symbolic gestures:

| Rank | Jurisdiction | Mean Score | Income | N Policies |
|:---|:---|---:|:---|---:|
| 1 | European Union | 1.42 | HI | 60 |
| 2 | Canada | 1.38 | HI | 15 |
| 3 | United Kingdom | 1.32 | HI | 72 |
| 4 | United States | 1.28 | HI | 84 |
| 5 | Colombia | 1.21 | UMI | 8 |

: Top 5 jurisdictions by capacity score {#tbl-cap-top5}

@tbl-cap-top5 reveals an unsurprising but important pattern: the top four positions are occupied by high-income jurisdictions with extensive AI policy ecosystems and sophisticated regulatory traditions. The **European Union** leads with a mean capacity score of 1.42, reflecting its comprehensive regulatory framework anchored by the AI Act and supported by extensive complementary policies. **Canada** (1.38) and the **United Kingdom** (1.32) demonstrate that smaller policy portfolios can achieve high average quality through careful policy design. The **United States** (1.28), despite the largest policy portfolio (84 documents), ranks fourth due to considerable variation across federal agencies and states — some US policies achieve top-tier scores while others function as brief announcements.

The fifth position, however, disrupts this high-income dominance. **Colombia** (1.21), an upper-middle-income country, outperforms numerous wealthy nations through a focused portfolio of eight well-designed policies. Colombia's achievement exemplifies a pattern explored extensively in @sec-cap-dynamics: several developing countries punch above their GDP-predicted weight through strategic policy choices rather than resource abundance. Notable developing-country performers beyond the top five include **Brazil** (consistently in the top 10) and **Kenya** (ranking above many European countries despite substantially lower GDP per capita).

These outliers prove analytically valuable precisely because they challenge the assumed tight coupling between wealth and governance capacity. If Colombia, Brazil, and Kenya can achieve strong implementation readiness despite resource constraints, this suggests that institutional design choices — clarity of objectives, establishment of coordination mechanisms, specification of authorities — matter more than fiscal capacity alone. We return to these efficiency frontier countries in subsequent chapters to understand what enables their outperformance.

### Correlation Structure {#sec-cap-correlations}

The preceding analyses treat the five capacity dimensions as separate constructs, examining how Clarity differs from Resources or how Accountability gaps vary by income. But are these truly distinct dimensions, or do they simply measure the same underlying governance quality factor with slightly different labels? The correlation structure among dimensions reveals whether our framework captures meaningful multidimensionality or redundantly measures a single latent construct.

![Correlation matrix across the five capacity dimensions. All dimensions are positively correlated, with the strongest link between C3 (Authority) and C5 (Coherence).](../data/analysis/paper1_capacity/fig_correlations.png){#fig-cap-correlations}

@fig-cap-correlations shows that the five capacity dimensions are indeed positively correlated, with pairwise correlations ranging from r = 0.45 to r = 0.75. This pattern indicates substantial shared variance — policies that score high on one dimension tend to score high on others, suggesting a common underlying governance quality factor. The strongest correlations link Authority (C3) with Coherence (C5) (r = 0.75), and Clarity (C1) with Authority (r = 0.70), reflecting natural implementation logic: policies with clear legal mandates more readily establish coordination mechanisms, and those with specific objectives more effectively define enforcement authorities.

However, the correlations remain well below 1.0, indicating that the dimensions maintain sufficient discriminant validity to justify separate measurement. A policy can score high on Clarity (well-defined objectives) while scoring low on Resources (no budget allocation), or achieve strong Coherence (cross-agency coordination) despite weak Accountability (no monitoring mechanisms). The moderate correlation structure suggests that our framework successfully captures distinct facets of implementation capacity rather than redundantly measuring a single dimension.

This structure receives formal confirmation through the principal component analysis presented in @sec-pca-nexus, which demonstrates that while capacity dimensions share approximately 66% of their variance through a general governance factor, they also exhibit sufficient independence to warrant separate analysis. The multidimensional framework thus provides richer diagnostic information than a single overall quality score would permit, enabling identification of specific capacity strengths and weaknesses that aggregate scores would obscure.
