---
title: "Literature Review"
---

## Theoretical Foundations {#sec-literature}

::: {.callout-note appearance="simple"}
**Chapter summary.** We situate our framework at the intersection of three literatures: (1) implementation science and state capacity theory, (2) the rapidly growing AI governance scholarship, and (3) regulatory capacity in the Global South. From these, we derive the Implementation Capacity–Equity (ICE) framework and our core hypotheses.
:::

### Implementation Science {#sec-lit-implementation}

The study of policy implementation has a long history. Beginning with Pressman and Wildavsky's [-@pressman1973] seminal observation that even well-designed programmes routinely fail to achieve their objectives, a half-century of research has sought to identify the conditions under which policies translate into practice. This literature is directly relevant to AI governance: understanding what separates policies that can be implemented from those that remain aspirational requires examining not just policy content but the institutional infrastructure supporting execution.

Implementation research can be broadely divided into three streams. Top-down approaches identify the formal design features that enable effective execution. Bottom-up approaches examine how frontline actors shape policy outcomes through discretionary decisions. State capacity theories connect implementation success to broader institutional and fiscal capabilities. Together, these streams provide the conceptual foundation for measuring whether AI policies possess the structural features necessary for implementation.

These foundational frameworks remain the standard analytical approach in contemporary policy research. Recent studies of digital governance, platform regulation, and algorithmic accountability continue to employ Mazmanian and Sabatier's implementation conditions and Lipsky's street-level bureaucracy framework, demonstrating their robustness across technological and temporal contexts. Sempé et al.'s [-@sempe2025] conceptual framework for evidence-informed policymaking exemplifies this continuity: their analysis of how capabilities, institutional structures, and actor networks shape policy processes builds directly on the implementation science tradition established four decades earlier. The core challenge these frameworks identified — ensuring that policy design enables institutional execution — has proven persistent regardless of policy domain.

Our Implementation Capacity–Equity (ICE) framework represents a natural continuation of this research tradition. Where recent frameworks like @sempe2025 examine how institutional capacity shapes the *creation* of evidence-informed policies, our framework assesses whether resulting policies possess the structural features necessary for *implementation*. Implementation failures in contemporary AI governance (ambiguous principles without enforcement mechanisms, coordination gaps across agencies, insufficient technical expertise) mirror the patterns documented decades earlier, confirming the enduring relevance of this theoretical tradition. By operationalizing classical implementation conditions for the AI governance context, we extend this analytical approach to a new policy domain while maintaining conceptual continuity with established theory.

#### Top-Down Approaches

The dominant strand in early implementation research took a top-down perspective. @mazmanian1983 identified six conditions for effective implementation: clear and consistent policy objectives, adequate causal theory linking intervention to outcomes, legal structuring of the implementation process, committed and skilled implementing officials, support of organised interest groups, and stable socioeconomic conditions.

These conditions map directly onto our measurement framework. Clarity of objectives corresponds to our *Clarity & Specificity* dimension; legal structuring to *Authority & Enforcement*; and socioeconomic conditions to *Resources & Budget*.

@sabatier1986 later synthesised top-down and bottom-up perspectives, arguing that both the formal structure of implementation and the strategies of implementing actors must be considered. This synthesis informs our dual attention to formal policy architecture (capacity dimensions) and substantive commitments (ethics dimensions).

The top-down framework remains particularly relevant for analyzing AI governance, where policy clarity directly affects implementation feasibility. Contemporary AI policies often suffer from the same clarity deficits Mazmanian and Sabatier identified: vague objectives ("promote trustworthy AI"), unclear causal theories linking interventions to outcomes, and ambiguous allocation of authority across multiple agencies. Our measurement of policy specificity and coherence operationalizes these classical implementation conditions for the AI governance context.

#### Bottom-Up Approaches

@lipsky1980 significantly shifted the focus of implementation research by examining "street-level bureaucrats" — the frontline workers who, through their discretionary decisions, effectively *make* policy. In the AI governance context, Lipsky's insight implies that even comprehensive legislation may fail if regulators lack the expertise, resources, or mandate to exercise oversight.

Our *Accountability & M&E* dimension captures this concern: policies that specify monitoring mechanisms, evaluation requirements, and feedback loops constrain discretion and create accountability pressure on implementing agents.

@hjern1982 extended the bottom-up critique by arguing that implementation structures rarely correspond to formal organisational charts. Instead, they emerge from networks of actors operating across institutional boundaries — a particularly relevant observation for AI governance, where responsibilities typically span multiple ministries, regulators, and international bodies.

The bottom-up perspective proves especially valuable for understanding AI governance implementation. Regulators tasked with enforcing "fairness" or "transparency" requirements face discretionary choices similar to those Lipsky described: translating vague mandates into concrete decisions with limited guidance. Data protection authorities interpreting GDPR's algorithmic accountability provisions, or competition regulators assessing AI-driven market power, exemplify the street-level bureaucrat role. Our Accountability dimension captures whether policies constrain this discretion through monitoring mechanisms and evaluation frameworks, or leave implementation to ad hoc regulatory interpretation.

#### State Capacity

A third stream, rooted in comparative political economy, examines the broader institutional conditions that determine implementation success. @grindle1996 identified four types of state capacity:

| Capacity Type | Our Dimension | Indicators |
|:---|:---|:---|
| Technical | Resources | Expertise, training, technology |
| Administrative | Authority | Organisational structures, legal mandate |
| Political | Coherence | Cross-ministry coordination |
| Fiscal | Resources | Budget allocation |

: Mapping state capacity types to our framework {#tbl-capacity-mapping}

@tbl-capacity-mapping illustrates how Grindle's state capacity taxonomy maps onto our measurement dimensions. Technical and fiscal capacity both inform our Resources dimension, as both expertise and funding are necessary for implementation. Administrative capacity corresponds to the Authority dimension, reflecting the legal mandate and organizational infrastructure required to enforce policies. Political capacity aligns with our Coherence dimension, capturing the coordination mechanisms needed when AI governance responsibilities span multiple agencies.

@fukuyama2013 argued that governance quality is conceptually distinct from both democracy and state capacity, and proposed measuring it through the quality of government outputs. Our scoring framework operationalises precisely this idea — measuring not *inputs* (e.g., GDP) but *outputs* (the quality of policy documents as indicators of institutional readiness).

More recently, @andrews2017 introduced the concept of "building state capability" through iterative adaptation, arguing against the imposition of "best practice" models from high-income countries. Their framework resonates with our finding that developing countries often achieve governance quality through different pathways than those taken by wealthy nations.

The state capacity framework has gained renewed relevance with the rise of AI governance, where technical expertise becomes a distinct capacity constraint. Traditional state capacity focused on fiscal resources and administrative structures, but AI governance requires specialized knowledge of machine learning, algorithmic systems, and technical standards. This creates capacity challenges even in high-income countries with strong administrative traditions but limited AI expertise in government. Our Resources dimension captures both traditional fiscal capacity (budget allocations) and these newer forms of technical capacity (expertise, training programs, technology infrastructure), recognizing that effective AI governance requires capabilities beyond those emphasized in earlier state capacity literature.

### AI Governance {#sec-lit-aigovernance}

The AI governance literature has experienced substantial growth since 2018, but remains predominantly descriptive and normative rather than analytical. This body of work has focused primarily on documenting the principles that appear in AI governance documents and mapping the diversity of regulatory approaches across jurisdictions. While this descriptive work provides essential context, it leaves a critical question unanswered: whether the policies being catalogued possess the institutional features necessary for implementation.

Three strands characterize this literature. The ethics mapping wave identified convergence around core principles but did not assess implementation feasibility. Studies of regulatory approaches documented cross-national variation in governance strategies but rarely examined capacity to execute those strategies. Throughout this literature runs a consistent gap: the absence of systematic measurement linking policy design to implementation readiness. This gap motivates our framework.

#### The Ethics Mapping Wave

@jobin2019 conducted the foundational mapping exercise, analysing 84 AI ethics guidelines and identifying convergence around five principles: transparency, justice/fairness, non-maleficence, responsibility, and privacy. @floridi2018 proposed the AI4People framework, adding beneficence and autonomy. @fjeld2020 extended the mapping to 36 documents from the Berkman Klein Center.

These studies established *what* principles appear in AI governance documents but did not assess *whether* the policies containing these principles can be implemented. As @hagendorff2020 observed, "the ethics of AI ethics" — the gap between principles and practice — remains the field's most pressing challenge.

#### Regulatory Approaches

A second strand examines the regulatory strategies adopted by different jurisdictions. @radu2021 mapped national AI strategies and identified four governance approaches: rights-based (EU), innovation-led (US), state-directed (China), and adaptive (UK). @bradford2020 theorised the "Brussels Effect" — the tendency for EU regulations to diffuse globally through market power — which we test empirically through our examination of how policies spread across countries.

::: {.callout-tip title="Definition: Brussels Effect"}
The Brussels Effect describes how the European Union's regulatory standards become de facto global norms through market mechanisms rather than formal treaties. When the EU adopts stringent regulations (such as GDPR for data protection), multinational firms often find it more efficient to adopt EU standards globally rather than maintain separate compliance systems. This creates upward regulatory convergence even in jurisdictions that did not participate in creating the rules. In AI governance, the Brussels Effect hypothesis predicts that EU frameworks (such as the AI Act) will shape policies worldwide.
:::

@smuha2021 framed the current landscape as a "race to AI regulation," arguing that the proliferation of national approaches risks fragmentation. @stix2021 proposed "actionable principles" as a bridge between high-level ethics and concrete regulation, a concept closely aligned with our operationalisation dimension.

#### The Missing Implementation Dimension

What the AI governance literature conspicuously lacks is any systematic measurement of *implementation readiness*. Studies describe regulatory architectures but do not assess whether they can function. They catalogue principles but do not examine whether institutions exist to enforce them.

This gap is remarkable given that implementation science — a well-established field with decades of theory and evidence — provides ready-made frameworks for exactly this analysis. Our study bridges this disciplinary gap.

### Regulatory Capacity in the Global South {#sec-lit-globalsouth}

The development literature offers a critical perspective on regulatory capacity in contexts where institutional resources are constrained. This literature challenges the assumption that regulatory sophistication automatically translates into governance quality. In contexts of limited state capacity, regulatory ambition can outstrip institutional reality — what @grindle2004 termed the need for "good enough governance." This insight is particularly relevant to AI governance, where developing countries face pressure to adopt comprehensive regulatory frameworks despite resource constraints.

A central concern in this literature is distinguishing genuine institutional development from what @andrews2017 call "isomorphic mimicry" — the adoption of institutional forms without underlying functionality. For AI governance, this raises the question of whether observed policy adoption in developing countries reflects actual capacity building or symbolic compliance with international norms. Our measurement framework is designed to detect exactly this distinction by separately scoring formal policy architecture and operational dimensions.

@andrews2017 documented how developing countries often adopt "best practice" institutions that look impressive on paper but lack the implementing infrastructure to function — a phenomenon they call "isomorphic mimicry." Our scoring framework is designed to detect exactly this pattern: policies that score high on formal architecture (Clarity, Coherence) but low on operational dimensions (Resources, Accountability).

::: {.callout-tip title="Definition: Isomorphic Mimicry"}
Isomorphic mimicry occurs when governments adopt the form of successful institutions from other countries without developing the underlying functionality. The institution appears similar (isomorphic) to international best practices but lacks the capacity to perform its intended function. This is common in contexts where international pressure, aid conditionality, or legitimacy concerns incentivize governments to signal modernity through policy adoption rather than implementation. In AI governance, isomorphic mimicry would manifest as policies that reference international frameworks and establish institutional structures but lack the resources, expertise, or enforcement mechanisms to function effectively.
:::

The question of whether developing countries are "copying" high-income regulatory models or developing indigenous approaches is central to our diffusion analysis (see @sec-cap-dynamics). If policy diffusion is primarily vertical (rich → poor), this supports the mimicry hypothesis. If horizontal (peer-to-peer), it suggests more organic adaptation.

### The ICE Framework {#sec-ice-framework}

Drawing on these three literatures, we propose the **Implementation Capacity–Equity (ICE) framework**, which links policy formulation quality to implementation readiness through five capacity dimensions and five ethics dimensions, moderated by country-level contextual factors. The framework synthesizes insights from implementation science (which identifies structural features enabling execution), AI governance scholarship (which documents principles and regulatory approaches), and development studies (which examines capacity constraints and institutional mimicry).

As established in the implementation science discussion above, our framework represents a natural continuation of the research tradition exemplified by @sempe2025. Where their evidence-informed policymaking framework examines how institutional capabilities, actor networks, and structural processes shape policy *formulation*, the ICE framework assesses whether resulting policies possess the features necessary for *implementation*. Both frameworks operationalize the same foundational insight from Mazmanian, Sabatier, Lipsky, and Grindle: institutional capacity determines whether policy designs translate into functional governance. By extending this analytical approach to AI governance, we maintain conceptual continuity with established implementation science while addressing a new policy domain.

The capacity dimensions operationalize Mazmanian and Sabatier's implementation conditions: Clarity (clear objectives), Resources (fiscal and technical capacity), Authority (legal mandate), Accountability (monitoring mechanisms), and Coherence (coordination across agencies). The ethics dimensions capture the substantive principles identified in the AI governance literature: Framework Depth, Rights Protection, Governance Mechanisms, Operationalisation, and Inclusion. Together, these ten dimensions provide a comprehensive assessment of whether AI policies possess both the structural infrastructure and substantive commitments necessary for effective governance.

The framework generates five testable hypotheses:

::: {#hyp-panel}

**H1 (Capacity-Development).** Countries with higher implementation capacity scores will demonstrate higher rates of AI technology adoption.

**H2 (Capacity Gap).** Developing countries will show lower implementation capacity scores than high-income countries, even controlling for policy quantity.

**H3 (Dimension Heterogeneity).** The capacity gap will be largest in the Resources dimension and smallest in the Clarity dimension.

**H4 (Sectoral Variation).** Implementation capacity gaps will vary by policy type, with larger gaps in resource-intensive binding regulation than in soft-law guidelines.

**H5 (Regional Clustering).** Implementation capacity will cluster by region, reflecting shared institutional legacies and patterns of policy learning across countries.

:::

We test these hypotheses in Parts I and II using complementary analytical approaches spanning descriptive, inferential, and comparative methods. The Capacity-Development hypothesis (H1) examines whether our framework captures meaningful variation in institutional readiness. The Capacity Gap hypothesis (H2) tests the conventional assumption of a North–South divide in governance quality. Dimension Heterogeneity (H3) explores whether resource constraints affect developing countries more than design choices. Sectoral Variation (H4) recognizes that different policy instruments face different implementation challenges. Regional Clustering (H5) tests whether governance approaches reflect geographic proximity and shared institutional histories more than income levels alone.
