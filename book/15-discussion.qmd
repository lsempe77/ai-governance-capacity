---
title: "Discussion"
---

## Implications for Policy and Research {#sec-discussion}

::: {.callout-note appearance="simple"}
**Chapter summary.** We discuss five implications of our findings: (1) the governance gap is universal, not just a developing-country problem; (2) GDP is not destiny; (3) text quality confounds everything; (4) ethics governance needs a different policy model than capacity building; and (5) peer-to-peer learning beats top-down technical assistance.
:::

### Implication 1: The Universal Governance Deficit

Perhaps the most sobering finding from analyzing 2,216 AI policies across 70+ jurisdictions is how pervasively weak global AI governance proves. The dominant narrative in policy discourse frames AI governance as a North-South challenge: developing countries lack capacity while wealthy nations demonstrate sophisticated frameworks. Our evidence fundamentally contradicts this narrative. The governance deficit proves **universal rather than concentrated** in developing countries, affecting policies across all income groups, regions, and governance traditions.

The statistics prove stark: **96.5% of AI policies worldwide score below 2.0/4.0** on implementation readiness, with 2.0 representing the minimal threshold for "moderate" governance where policies specify basic implementation infrastructure. This means that the overwhelming majority of global AI policies — including those from the United States, European Union, China, and other technological leaders — demonstrate aspirational rather than operational governance. They articulate intentions, values, and objectives without establishing the institutional mechanisms needed to translate aspiration into implementation.

This universal weakness concentrates particularly in **Accountability (C4)**, which scores lowest globally at mean 0.48/4.0. Accountability mechanisms — monitoring procedures, evaluation frameworks, enforcement authorities, feedback loops, and course-correction processes — prove systematically absent from AI policies worldwide. Even policies scoring reasonably well on other dimensions (clear objectives in C1, designated authorities in C3, coordination mechanisms in C5) typically lack the accountability infrastructure needed to ensure that designated authorities actually implement stated objectives and that coordination mechanisms function as intended.

The absence of accountability mechanisms creates governance ecosystems where policies can fail without detection or correction. Without monitoring, policymakers cannot determine whether AI systems comply with regulations. Without evaluation, governments cannot assess whether governance frameworks achieve intended outcomes. Without enforcement authorities, violations carry no consequences. Without feedback loops, policies cannot adapt to technological change or implementation challenges. The systematic weakness in C4 thus undermines even policies demonstrating strength on other dimensions.

The universality of this deficit — affecting high-income and developing countries alike — suggests structural explanations rather than country-specific failures. **Monitoring and evaluation infrastructure proves difficult and expensive** to establish, requiring technical expertise, sustained funding, and political commitment that most jurisdictions struggle to maintain. **Industry resistance** to monitoring creates political barriers, with technology companies arguing that oversight stifles innovation and competitive advantage. **Technical challenges** complicate evaluation: assessing algorithmic fairness, measuring system robustness, and auditing AI decision-making require sophisticated methods that few governments have developed. **Political incentives** favor policy announcements over implementation oversight: producing new AI strategies generates favorable media coverage, while establishing monitoring frameworks involves tedious institutional development offering fewer political rewards.

**Policy recommendation**: Rather than producing more AI strategy documents, national AI councils, or aspirational frameworks, governance reforms should prioritize building monitoring and evaluation infrastructure. This includes establishing **annual review requirements** mandating that agencies responsible for AI governance publish progress reports; defining **published KPIs** (key performance indicators) enabling external observers to track implementation; creating **independent evaluation mechanisms** ensuring that assessments come from entities without institutional incentives to overstate success; and developing **technical audit capabilities** allowing governments to verify algorithmic compliance with governance requirements. The universal governance deficit requires universal attention to implementation readiness rather than continued emphasis on normative articulation that already exceeds operational capacity.

### Implication 2: GDP Is Not Destiny

One of this study's most robust and consequential findings challenges a foundational assumption in development economics and international policy: that governance quality emerges primarily from economic prosperity. Across all our analyses — OLS regression, multilevel models, quantile regression, Tobit models, efficiency frontier analysis — GDP per capita explains between **1.5% (ethics) and 3.5% (capacity)** of country-level governance variation. These trivial R² values indicate that knowing a country's national wealth provides almost no information about its AI governance quality.

The efficiency frontier analysis visualizes this finding dramatically. Countries anchoring the frontier — achieving maximum governance quality per dollar of GDP — include **Rwanda** (2.30-3.10 efficiency ratio), **Kenya**, **Uganda**, and **Brazil**. These countries demonstrate sophisticated AI governance frameworks despite per-capita GDPs ranging from $800 (Rwanda) to $9,000 (Brazil), achieving implementation readiness and ethical sophistication that many countries 5-20 times wealthier do not. Rwanda's comprehensive national AI strategy articulates clear objectives, designates authorities, allocates (modest) resources, establishes coordination mechanisms, and incorporates stakeholder inclusion — scoring higher on multiple dimensions than policies from several OECD countries.

Conversely, several wealthy countries significantly **underperform** their GDP-predicted levels. **Kazakhstan**, despite substantial natural resource wealth producing per-capita GDP exceeding $10,000, demonstrates governance scores 0.56-0.75 points below predictions. **South Korea**, a technological powerhouse with sophisticated AI industry and per-capita GDP exceeding $35,000, produces policies scoring below what its wealth would predict. **Portugal**, a European Union member with strong institutional capacity, similarly underperforms. These cases prove that wealth provides necessary resources but insufficient motivation: governance quality emerges from political choices to prioritize AI governance, not automatically from fiscal abundance.

This finding directly challenges the **"development first, governance later"** sequencing implicit in much international development discourse. Development agencies and international financial institutions frequently argue that developing countries should prioritize economic growth, infrastructure development, and basic service delivery before investing in sophisticated governance frameworks like AI regulation. The implicit model holds that governance capacity constitutes a "luxury good" requiring economic security before adoption — countries must first build wealth, then use that wealth to construct governance institutions.

Our evidence fundamentally contradicts this sequencing. If GDP determined governance quality, we would observe strong positive correlations (r > 0.60) and high explanatory power (R² > 0.30). Instead, we find weak correlations (r ≈ 0.20 for capacity, r ≈ 0.12 for ethics) and trivial explanatory power (R² = 0.015-0.035). The relationship proves so weak that GDP essentially functions as noise rather than signal for predicting governance scores.

The finding suggests that governance capacity-building is a **complement to economic development**, not a **consequence of it**. Countries can and should develop AI governance frameworks concurrently with economic development rather than sequentially after it. Rwanda's governance success did not require waiting for per-capita GDP to reach high-income thresholds ($13,845+ in 2024 World Bank classification) — it emerged from deliberate policy choices to prioritize governance infrastructure despite modest fiscal resources. Brazil similarly achieved ethics governance sophistication when per-capita GDP hovered around $8,000, well below the high-income threshold.

The mechanisms enabling governance quality despite limited resources prove instructive. **Technical assistance** from international organizations (UN, African Union, OECD) provides expertise that countries need not develop indigenously. **Policy transfer** allows countries to adapt successful frameworks from peers facing similar constraints rather than inventing governance models from scratch. **Civil society engagement** enables inclusive governance development through stakeholder consultation that substitutes for extensive government capacity. **Regional cooperation** facilitates shared monitoring infrastructure and mutual learning that individual countries could not afford independently.

**Policy recommendation**: Development agencies (World Bank, regional development banks, bilateral aid programs) should fund AI governance capacity-building **regardless of recipient-country income levels**, abandoning the implicit sequencing model requiring economic development first. These interventions should target **dimension-specific gaps** revealed through diagnostic assessments — particularly Resources (C2) and Accountability (C4) where capacity constraints prove most binding — rather than generic "capacity building" programs. Technical assistance should emphasize **peer learning** from frontier countries at similar income levels (Rwanda for African countries, Brazil for Latin America, Vietnam for Southeast Asia) rather than promoting wealthy-country models assuming infrastructure that recipients lack. The evidence proves that governance sophistication emerges from focused institutional design and political commitment rather than from fiscal abundance, making governance capacity-building immediately actionable for all countries.

### Implication 3: The Measurement Problem

Perhaps our most methodologically unsettling finding emerges from the robustness analysis in @sec-robustness: the income-group gap **vanishes completely** when analyses restrict to good-quality texts (≥500 words). This finding proves more than a technical footnote about our specific study — it exposes a fundamental measurement challenge affecting all text-based comparative governance research, with profound implications for how we understand global governance inequality.

For the full corpus of 2,216 policies, high-income countries demonstrate modest but statistically significant advantages: Cohen's d = 0.30 for capacity, d = 0.20 for ethics. These effect sizes, while "small" by psychological science standards, suggested systematic differences warranting attention. But restricting analysis to the 948 policies with adequate text quality (≥500 words allowing meaningful assessment) produces dramatically different results: d = 0.04 for capacity (87% reduction, non-significant), d = -0.09 for ethics (gap elimination and sign reversal, non-significant). The apparent governance divide thus reflects primarily **documentation quality rather than governance quality**.

The mechanism operates through multiple pathways. **Availability effects** arise because high-income countries more consistently publish complete policy documents as accessible PDFs, while developing countries' policies more often appear in international databases as brief summaries, metadata entries, or excerpted descriptions. The OECD.AI Observatory's comprehensive coverage procedures cannot always obtain full texts for all jurisdictions, particularly those with limited English documentation, restricted document access, or policies circulated only through national-language channels. When a policy exists only as a 75-word summary describing its existence and general purpose, our LLM scoring ensemble naturally detects fewer governance features than when scoring a 7,500-word comprehensive strategy, mechanically depressing scores regardless of the policy's actual sophistication.

**Length effects** compound this: longer documents provide more opportunities to mention authorities, specify resources, articulate principles, and describe procedures. A policy might allocate $10M for implementation, but if the available text mentions this only briefly while the bulk discusses background and objectives, the LLM scores may not detect the resource allocation. A comprehensive document detailing the same $10M allocation across multiple sections naturally scores higher despite equivalent resource commitment. The **detail gradient** thus creates measurement artifacts: policies described with greater textual specificity score higher even when underlying governance proves equivalent.

**Translation effects** add a third layer: policies originally drafted in French, Spanish, Portuguese, Arabic, or other languages and then machine-translated to English for analysis may lose nuance, specificity, and institutional detail that human translation would preserve. Technical governance terminology often lacks direct translation equivalents, forcing machine translation systems to use generic terms that score lower than precise English originals. Developing countries with colonial histories featuring French or Spanish official languages face particular disadvantages in English-centric international governance databases.

The implications extend far beyond our study. **Any text-based governance analysis** — whether using LLMs, traditional NLP methods, or human coding — faces the same fundamental confound: what researchers measure through policy documents reflects document availability and drafting resources as much as underlying institutional reality. Studies comparing regulation quality, legislative sophistication, treaty compliance, or policy innovation across countries will systematically advantage jurisdictions that invest in producing detailed, publicly accessible, English-language policy documentation.

This measurement challenge proves particularly insidious because it **correlates with the outcome of interest**: wealthy countries tend toward better documentation *and* stronger governance capacity, making it analytically difficult to separate genuine governance differences from measurement artifacts. Standard econometric approaches (controlling for text length, including document type fixed effects) provide partial solutions but cannot fully eliminate the confound when text availability itself reflects governance capacity.

The finding also raises uncomfortable questions about **existing governance research**. How many published findings about North-South governance gaps, democratic advantage in policy sophistication, or institutional quality differences reflect genuine governance variation versus documentation quality? Studies using policy databases (OECD collections, UN repositories, regional organization libraries) to compare governance sophistication across countries may systematically overestimate high-income advantages if they fail to control for text availability and quality.

**Methodological recommendation**: Future comparative governance research must explicitly address document availability and quality as potential confounds. At minimum, studies should **report results stratified by text length and quality tiers**, showing whether findings persist when restricted to well-documented policies. Ideally, researchers should **weight observations by inverse text quality** to correct for documentation advantages, or **match comparison groups** on text characteristics before assessing governance differences. International governance databases should **standardize documentation quality requirements**, refusing to include policies available only as brief summaries and instead investing resources in obtaining complete texts through targeted outreach, translation services, and partnerships with national governments. The measurement problem will not disappear through methodological sophistication alone — it requires improved data infrastructure ensuring comparable documentation quality across all jurisdictions before valid comparative analysis becomes possible.

### Implication 4: Ethics ≠ Capacity

The two-factor PCA structure documented in @sec-pca-nexus provides empirical validation for treating capacity and ethics as related but distinct constructs, with 65.9% shared variance (general governance factor) and 12.8% independent variance (capacity-ethics separation). But beyond this structural validation, capacity and ethics exhibit strikingly different relationships with national wealth, temporal dynamics, and international diffusion patterns. These asymmetries prove theoretically coherent and practically consequential: they imply that **capacity and ethics require fundamentally different policy interventions** with distinct theories of change.

The contrast table summarizes key asymmetries:

| | Capacity | Ethics |
|:---|:---|:---|
| GDP effect (OLS) | Modest ($\beta = 0.09$*) | Illusory ($\beta = 0.06$*) |
| GDP effect (quantile) | Inverted-U | **Zero at all quantiles** |
| Convergence | Stable gap | Gap narrowing (HI declining) |
| Adoption gap | Small (86% vs 98%) | Large (72% vs 100%) |

: Capacity vs. ethics: asymmetric patterns {#tbl-cap-eth-contrast}

@tbl-cap-eth-contrast reveals four fundamental asymmetries distinguishing capacity from ethics governance:

**GDP Effects**: While GDP shows a modest positive effect on capacity in OLS regression (β = 0.09, p < .05), this coefficient proves fragile — it shrinks substantially in multilevel models and disappears entirely when restricted to good-quality texts. For ethics, the OLS coefficient (β = 0.06, p = .002) proves entirely illusory: quantile regression demonstrates **zero effect at every quantile** of the positive-score distribution, indicating that the OLS significance operates entirely through the extensive margin (whether any ethics content exists) rather than affecting score quality conditional on having content. This asymmetry makes theoretical sense: capacity dimensions like Resources (C2) require fiscal allocations, technical expertise, and administrative infrastructure that wealth facilitates, while ethics dimensions like Rights Protection (E2) and Framework Depth (E1) require normative clarity and political commitment orthogonal to GDP.

**Convergence Dynamics**: Capacity gaps remained **stable** from 2017 to 2025, with high-income and developing countries showing parallel temporal trends neither widening nor narrowing over time. Ethics gaps show significant **convergence** (−0.038 points per year, p = .018), but through an unexpected mechanism: high-income countries declining (−0.023/year, p = .001) while developing countries show modest non-significant improvement (+0.016/year). The contrasting dynamics suggest different diffusion mechanisms: capacity infrastructure proves difficult to build quickly through policy learning alone (requiring sustained institutional development), while ethical principles diffuse more readily through international frameworks, peer learning, and norm cascades but prove vulnerable to "ethics fatigue" or shifts toward operational regulation in mature governance systems.

**Adoption Gaps**: The coverage gap (what proportion of countries have adopted any governance) proves **small for capacity** (86% of developing countries versus 98% of high-income countries by 2025) but **large for ethics** (72% versus 100%). This reversed pattern indicates that capacity governance proves more universally adopted — even countries with minimal governance establish some implementation infrastructure — while ethics governance remains absent entirely from 28% of developing countries. The larger ethics adoption gap suggests that political, cultural, or institutional barriers prevent some countries from engaging ethical AI governance frameworks despite widespread international norm-setting through UNESCO, OECD, and other instruments.

**Quantile Patterns**: Capacity shows an **inverted-U relationship** with GDP across quantiles: positive effects at low quantiles (helping countries move from zero to minimal capacity), stronger effects at medium quantiles, weakening at high quantiles where governance sophistication depends less on resources. Ethics shows **flat zero effects** across all quantiles, confirming that GDP contributes nothing to ethics quality regardless of where policies fall in the score distribution. This distinction reinforces that capacity building benefits from targeted resource investments while ethics governance depends on factors orthogonal to wealth.

These asymmetries carry direct implications for international AI governance initiatives, development programs, and technical assistance:

**Capacity building** benefits from traditional development interventions: targeted resource allocation (budgets for regulatory agencies, salaries for technical staff, funding for monitoring systems), technical expertise transfer (training programs, secondment arrangements, twinning partnerships), legal infrastructure development (drafting model legislation, establishing administrative procedures, creating enforcement mechanisms), and institutional design support (organizational structure recommendations, coordination mechanism templates, accountability system frameworks). These interventions align with standard development practice because capacity governance exhibits some GDP sensitivity and requires fiscal investments that wealth facilitates.

**Ethics governance** requires fundamentally different interventions independent of resource constraints: political will cultivation through civil society advocacy and constituency building, stakeholder engagement processes enabling meaningful participation from affected communities, normative deliberation facilitating collective reasoning about values and principles, international framework dissemination ensuring awareness of UNESCO/OECD ethical standards, peer learning networks connecting countries to demonstrate that ethics sophistication is achievable regardless of income, and technical assistance on operationalization showing how to translate ethical principles into enforceable requirements. These interventions emphasize political processes, democratic accountability, and normative clarity rather than fiscal transfers or expertise deployment.

The asymmetry suggests that **integrated AI governance programs should establish separate work streams** for capacity and ethics with distinct theories of change, implementation modalities, and success metrics. Capacity work streams can follow traditional technical assistance models: needs assessments identifying dimension-specific gaps, resource mobilization providing funding and expertise, institutional development building agencies and procedures, and monitoring evaluating whether capacity infrastructure operates as intended. Ethics work streams require process-oriented approaches: multi-stakeholder dialogues enabling normative deliberation, civil society strengthening ensuring advocacy capacity, rights protection training building awareness of human rights obligations, and peer learning facilitating South-South knowledge exchange.

The evidence contradicts both "capacity-first" sequencing (arguing that countries must build implementation infrastructure before engaging ethics) and "ethics-first" sequencing (arguing that normative foundations must precede operational development). Countries can enter through either construct depending on institutional starting points, political opportunities, and stakeholder priorities. The PCA finding that 10.9% of policies show high capacity/low ethics while 10.6% show low capacity/high ethics confirms that no universal pathway exists — both routes prove viable for reaching comprehensive governance (the high-high quadrant comprising 45.5% of policies).

**Policy recommendation**: International AI governance initiatives — including multilateral programs through UN agencies, regional organizations, bilateral development assistance, and philanthropic funding — should treat ethics and capacity as **separate work streams with different theories of change** rather than assuming that building one automatically generates the other or that universal templates apply regardless of construct. Diagnostic assessments should evaluate capacity and ethics separately to identify which construct requires priority attention in specific country contexts. Intervention design should match modality to construct: traditional technical assistance and resource transfers for capacity gaps, political process support and normative dialogue for ethics gaps. Success metrics should recognize that capacity and ethics improve through different mechanisms on different timelines: capacity requires sustained institutional development over years, while ethics can shift more rapidly through norm adoption but proves vulnerable to regression absent political commitment.

### Implication 5: Peer-to-Peer Learning

Our diffusion analysis across both capacity (@sec-cap-diffusion) and ethics (@sec-eth-diffusion) chapters reveals a striking and consistent pattern: **98% of policy diffusion operates horizontally** within income groups rather than vertically from wealthy to developing countries. This finding fundamentally challenges the dominant "Brussels Effect" hypothesis that has shaped AI governance discourse and development assistance practice over the past decade.

The Brussels Effect thesis, articulated most influentially by Bradford [-@bradford2020], holds that EU regulations diffuse globally through market mechanisms and regulatory competition. Multinational corporations facing stringent EU standards (GDPR for data protection, proposed AI Act for algorithmic regulation) find it efficient to adopt these standards worldwide rather than maintaining separate compliance regimes for different markets. Developing countries then import EU frameworks either to attract foreign investment, signal regulatory sophistication to international partners, or simply because EU models provide ready-made templates requiring less indigenous development. This vertical cascade supposedly flows from sophisticated to developing regulatory regimes, with wealthy countries (particularly the EU) functioning as norm entrepreneurs whose innovations spread downward through demonstration effects and power asymmetries.

Our evidence contradicts this hypothesis across multiple dimensions. The **adoption timing analysis** shows that high-income countries adopt AI governance policies only 1.2-1.3 years earlier than developing countries — a modest lag representing 15 months rather than the multi-year cascades characterizing earlier regulatory diffusion episodes. By 2025, adoption rates have largely converged (98-100% for high-income, 72-86% for developing depending on construct), indicating that the adoption gap reflects timing rather than ultimate adoption likelihood. More tellingly, the **98% horizontal diffusion rate** indicates that when countries adopt AI governance frameworks, they look primarily to peers within their income groups rather than importing wealthy-country models.

We classify diffusion as horizontal when policies reference, cite, or temporally follow frameworks from countries at similar income levels, and as vertical when developing countries explicitly adopt EU AI Act provisions, implement GDPR-inspired data protection regimes, or reference wealthy-country strategies as templates. The 98% horizontal classification indicates that such vertical transfers prove rare: developing countries develop policies reflecting their own institutional priorities, regulatory traditions, stakeholder consultations, and political contexts rather than copying high-income frameworks wholesale.

Several mechanisms explain why horizontal diffusion dominates vertical transfers despite Brussels Effect theory. **Institutional compatibility** proves crucial: developing countries recognize that wealthy-country governance models assume infrastructure they lack. The EU AI Act presumes sophisticated data protection authorities with technical capacity to audit algorithmic systems, judicial infrastructure enabling appeals of automated decisions, and administrative resources for ongoing compliance monitoring. Directly transplanting such frameworks to countries lacking these institutional foundations produces "isomorphic mimicry" ([@andrews2013]) — superficial policy adoption without functional implementation. Peer learning from countries facing similar institutional constraints proves more productive than importing frameworks assuming resources and expertise unavailable.

**Political economy considerations** also drive horizontal diffusion: developing countries may resist perceived "regulatory imperialism" from wealthy nations, preferring to develop indigenous approaches maintaining policy autonomy. Civil society organizations in developing countries often criticize vertical knowledge transfer as neocolonial, arguing that wealthy-country frameworks embed values, priorities, and trade-offs reflecting high-income contexts rather than developing-country needs. African countries emphasizing AI for development and social service delivery may find Asian peers' governance models more relevant than European frameworks emphasizing consumer protection and competition policy.

**Technical assistance infrastructure** limitations compound these factors: while European Commission programs, OECD initiatives, and bilateral assistance provide some vertical knowledge transfer, developing countries lack dense exposure to wealthy-country governance development. Regional organizations (African Union, ASEAN, OAS) and South-South cooperation networks (including through UNDP, World Bank, regional development banks) facilitate more intensive peer learning through regional workshops, policy lab exchanges, and collaborative development processes. The transaction costs of learning from geographically and institutionally proximate peers prove lower than engaging distant wealthy countries.

**Empirical validation** from efficiency frontier analysis reinforces horizontal diffusion findings: countries anchoring the frontier include Rwanda, Kenya, Brazil, and Uganda rather than traditional regulatory leaders. These frontier countries demonstrate that governance sophistication emerges from institutional design and political commitment rather than from wealth or technological advancement, making them more credible and relevant exemplars for other developing countries than wealthy nations whose governance sophistication might be attributed to resources unavailable elsewhere.

The horizontal diffusion pattern proves consistent across capacity and ethics, reinforcing that it represents a fundamental feature of AI governance diffusion rather than construct-specific artifact. For both implementation infrastructure and normative frameworks, countries look primarily to peers rather than wealthy exemplars.

**Practical implications** for development assistance and international governance initiatives prove substantial. Traditional North-South technical assistance models assuming that wealthy countries should transfer knowledge and templates to developing countries prove misaligned with actual diffusion patterns. If countries learn primarily from peers, development interventions should **facilitate regional peer-learning networks** rather than promoting universal "best practice" models from OECD countries. This implies:

**Regional governance labs** where countries at similar income levels share experiences, jointly develop frameworks addressing shared challenges, and collaboratively troubleshoot implementation difficulties. The African Union's AI Continental Strategy development process exemplifies this approach, bringing together African countries to develop indigenous governance frameworks rather than importing European models.

**South-South cooperation platforms** enabling developing countries to exchange governance innovations, successful adaptations, and institutional solutions without wealthy-country intermediation. Brazil-Africa AI governance partnerships, ASEAN AI governance coordination, and Latin American regional AI policy networks demonstrate that peer learning operates successfully without Northern facilitation.

**Frontier country showcasing** highlighting governance successes from Rwanda, Kenya, Brazil, Vietnam, and other developing countries achieving implementation readiness despite resource constraints. Making these cases visible counters narratives positioning governance sophistication as requiring first-world resources and demonstrates achievable pathways for peer countries.

**Translation and adaptation support** helping countries learn from regional peers in shared languages through culturally appropriate modalities. Rather than funding Northern consultants to deliver generic recommendations, development resources could support regional expertise networks, knowledge management platforms, and practitioner exchanges among peer countries.

**Diagnostic tools** enabling countries to identify which peers face similar governance challenges, institutional contexts, and resource constraints, facilitating targeted peer learning rather than assuming universal templates apply across development contexts.

The Brussels Effect may operate in specific commercial domains where market integration and corporate compliance incentives create vertical diffusion pressures, but it does not characterize AI governance adoption patterns globally. The evidence supports a **"peer learning effect"** where horizontal knowledge transfer within income groups dominates vertical cascades from wealthy regulatory leaders.

**Policy recommendation**: International organizations including UN agencies, World Bank, regional development banks, OECD, and bilateral development programs should redesign technical assistance to facilitate **regional peer-learning networks** rather than promoting universal best-practice models from wealthy countries. This requires shifting resources from North-South expert deployment toward South-South knowledge exchange, investing in regional governance labs bringing together peer countries, showcasing frontier country successes demonstrating governance achievability despite resource constraints, supporting translation and localization enabling peer learning in regional languages and cultural contexts, and developing diagnostic tools helping countries identify relevant peer exemplars facing similar institutional challenges. The evidence proves that countries learn most effectively from peers demonstrating that governance sophistication proves achievable within similar resource and institutional constraints rather than from wealthy exemplars whose success might be attributed to advantages unavailable elsewhere.

### Limitations {#sec-limitations}

While this study provides the most comprehensive assessment of global AI governance capacity to date, multiple limitations constrain our conclusions and suggest caution in extrapolating findings beyond our specific corpus and methodology. Acknowledging these limitations proves essential for valid inference and for identifying priorities for future research addressing current gaps.

**1. Selection bias in corpus coverage**: The OECD.AI Policy Observatory may systematically over-represent certain policy types, jurisdictions, and governance approaches. Countries with more international engagement — OECD members, G20 participants, UNESCO member states active in AI ethics deliberations — receive better coverage than isolated or less internationally visible jurisdictions. English-speaking countries and those with strong diplomatic representation in Paris (OECD headquarters) likely receive more comprehensive coverage. Policy types emphasized in OECD countries (national strategies, binding regulations, ethics frameworks) may be overrepresented relative to governance modalities more common elsewhere (customary practices, informal coordination, sector-specific standards). This selection bias likely inflates apparent governance sophistication globally: the policies analyzed represent the more visible, internationally engaged, and formally documented subset of global AI governance rather than a representative sample of all governance activity. Countries and regions underrepresented in the Observatory may demonstrate different capacity-ethics relationships, income-group patterns, or diffusion dynamics than observed in our sample.

**2. Text-as-proxy for implementation**: Our methodology measures governance capacity through **policy text** rather than through **implementation outcomes** or **institutional performance**. A policy scoring 3.5/4.0 on our capacity dimensions may specify clear objectives, designate authorities, allocate resources, establish accountability mechanisms, and create coordination frameworks in text, yet fail completely in practice due to political interference, bureaucratic capture, resource diversion, or technical incompetence. Conversely, countries with modest text-based scores might achieve effective governance through informal mechanisms, customary practices, or institutional capabilities not reflected in formal policy documents. The text-implementation gap likely varies systematically: wealthy countries with strong rule-of-law traditions may show higher text-implementation correspondence than countries where formal policies often remain aspirational. This limitation means our scores reflect "governance capacity on paper" rather than "governance capacity in practice," requiring validation against outcome measures to assess whether high-scoring policies actually function as intended.

**3. LLM reliability and bias**: While our inter-rater reliability proves excellent (ICC = 0.827 for the three-model ensemble, see @sec-scoring), LLMs may share systematic biases from overlapping training data, common architectural assumptions, or correlated failure modes. All three models in our ensemble (Claude Sonnet 4, GPT-4o, Gemini Flash 2.0) train on internet-sourced text corpora emphasizing English-language, Western-authored governance documents, potentially biasing scoring toward frameworks reflecting OECD regulatory traditions. Models might systematically undervalue governance approaches characteristic of non-Western traditions (consensus-based decision-making, community-oriented accountability, relational rather than individual rights conceptions). **Prompt sensitivity** adds another layer: alternative prompt formulations, rubric presentations, or scoring instructions might produce different results, and we cannot fully assess this sensitivity without extensive ablation studies. **Human validation on a larger sample** would strengthen confidence: our current validation involves human coding of 200 policies, but extending this to 1,000+ policies would better characterize human-LLM agreement and identify systematic divergences requiring methodological adjustment.

**4. Static snapshot limitations**: Governance capacity evolves rapidly as countries adopt new policies, revise existing frameworks, establish agencies, and develop institutional capabilities. Our scores reflect documents available as of **early 2026** and may already be outdated for jurisdictions undergoing rapid governance development. Countries receiving low scores in our analysis may have adopted comprehensive new strategies in late 2025 or early 2026 not yet reflected in the Observatory. This temporal limitation proves particularly acute for fast-moving governance domains like AI where policy development accelerates. The snapshot nature prevents assessing policy stability, institutional persistence, or governance resilience — we cannot determine whether high-scoring policies represent durable institutional achievements or temporary political commitments vulnerable to reversal. Longitudinal analysis tracking the same countries and policies over time would reveal governance trajectories, convergence dynamics, and stability patterns our cross-sectional design cannot address.

**5. English language bias**: Non-English policies may be underrepresented in the Observatory or analyzed through machine translation, potentially affecting scores systematically. Developing countries with French, Spanish, Portuguese, Arabic, or other official languages face particular disadvantages if their policies undergo machine translation losing institutional nuance, legal precision, or governance sophistication that English-native policies retain. Technical governance terminology often lacks direct translation equivalents, forcing generic substitutes that score lower than precise originals. **Measurement invariance** across languages remains untested: we cannot confirm that a policy scoring 2.5/4.0 in English-native and machine-translated versions reflects equivalent governance quality versus translation artifacts. Regional patterns showing lower scores in Francophone Africa or Spanish-speaking Latin America relative to Anglophone regions might partially reflect language processing limitations rather than genuine governance differences. Future research should incorporate native-language human coding for subsample validation, testing whether machine-translated and English-native policies show comparable score distributions controlling for actual governance quality.

**6. Endogeneity and causal inference**: We cannot identify **causal effects** of GDP, regime type, or other predictors on governance scores because these variables are jointly determined through complex historical, political, and institutional processes. Countries with high GDP may demonstrate strong governance capacity because wealth enables institutional development (causal interpretation), or because strong institutions facilitate economic growth producing high GDP (reverse causality), or because unobserved factors (colonial history, geographic advantages, human capital) jointly determine both GDP and governance capacity (confounding). Our regression analyses document **associations** rather than **causal effects**, requiring interpretive caution about policy interventions. The finding that GDP explains only 1.5-3.5% of governance variation indicates that GDP-governance association proves weak regardless of causality, but determining whether raising GDP would improve governance (causal effect) versus whether governance and GDP share common causes (confounding) requires identification strategies (instrumental variables, natural experiments, difference-in-differences) unavailable in our cross-sectional observational design. The endogeneity limitation means that even our most robust findings about GDP-governance relationships cannot definitively guide policy interventions without additional causal evidence from experimental or quasi-experimental research.

### Future Research {#sec-future-research}

This study opens multiple research directions extending, validating, and deepening understanding of AI governance capacity globally. Five priorities emerge as particularly consequential:

**1. Outcome validation through external measures**: The most critical extension involves correlating our capacity scores with **external measures of AI governance effectiveness** to validate that text-based capacity assessments predict real-world implementation and governance outcomes. Candidate validation measures include: **AI adoption rates** (do countries with higher capacity scores show greater AI deployment in government services, indicating that governance infrastructure enables adoption?); **regulatory enforcement actions** (do high-capacity jurisdictions actually investigate violations, impose penalties, and require remediation?); **algorithmic auditing activity** (do countries conduct technical assessments of AI systems as governance frameworks specify?); **stakeholder satisfaction surveys** (do civil society organizations, industry actors, and affected communities perceive governance as effective?); **compliance indicators** (do AI developers and deployers actually implement requirements specified in governance frameworks?); and **governance outcome metrics** (do countries achieve objectives like reducing algorithmic discrimination, improving AI transparency, or ensuring accountability?). Establishing that capacity scores predict these outcome measures would dramatically strengthen claims about governance quality, while finding weak score-outcome correspondence would suggest that text-based capacity assessment captures aspirational documentation rather than functional implementation. This validation research requires longitudinal data collection tracking governance outcomes over 3-5 year periods allowing capacity-building interventions to mature into measurable impacts.

**2. Causal identification through quasi-experimental designs**: Our cross-sectional observational design documents associations but cannot identify causal effects of governance interventions, GDP growth, or regime change on capacity development. Future research should exploit **quasi-experimental variation** to identify causal impacts: **policy adoption as treatment** (using staggered adoption timing across jurisdictions in difference-in-differences frameworks to estimate whether adopting national AI strategies causally improves governance capacity); **technical assistance program evaluation** (comparing capacity development in countries receiving World Bank AI governance assistance versus matched controls); **regional organization membership effects** (using discontinuity designs where countries just above/below membership thresholds for EU, African Union, or ASEAN receive differential governance support); **democratic transitions** (examining whether regime changes from autocracy to democracy causally improve ethics governance controlling for economic trends); and **resource windfalls** (testing whether natural resource discoveries or foreign aid surges causally improve capacity scores, validating GDP-capacity relationships). These causal identification strategies require creative research designs exploiting institutional features, policy timing variation, or exogenous shocks creating plausibly random assignment to governance-affecting treatments. The endogeneity pervasive in cross-sectional governance research necessitates such approaches for valid causal inference about what interventions actually improve AI governance capacity.

**3. Implementation science through case studies**: While our study characterizes what governance frameworks say on paper, understanding **how governance actually operates** requires deep institutional case studies examining implementation processes, bureaucratic politics, stakeholder engagement, and organizational behavior. Comparative case research should select countries spanning the efficiency frontier (Rwanda, Brazil), underperformers (Kazakhstan), and typical cases, conducting **process tracing** to identify mechanisms enabling or blocking capacity development. Key questions include: What political conditions enable countries to build accountability mechanisms despite weak GDP? How do frontier countries achieve ethics governance sophistication with modest resources? Why do wealthy countries sometimes underperform predictions despite fiscal capacity? What institutional features distinguish policies achieving high text-implementation correspondence from those remaining aspirational? How do bureaucratic agencies actually use AI governance frameworks in regulatory decision-making? Case study evidence would complement our quantitative findings by revealing causal mechanisms, contextual factors, and institutional processes determining whether formal governance capacity translates into functional implementation. Mixed-methods research integrating quantitative capacity scoring with qualitative case analysis would provide comprehensive understanding combining breadth (patterns across 70+ countries) with depth (mechanisms within specific contexts).

**4. Temporal dynamics through panel analysis**: Our cross-sectional design provides a 2026 snapshot but cannot track how countries' governance capacity evolves over time, whether capacity-building interventions produce sustained improvements, or whether governance gaps widen or narrow longitudinally. Future research should construct **panel datasets** tracking the same countries annually from 2017-2030+, enabling: **growth curve modeling** identifying which countries show rapid capacity development versus stagnation; **convergence analysis** testing whether income-group gaps narrow over time as predicted by diffusion theory; **policy learning assessment** examining whether later-adopting countries learn from early movers' experiences, producing higher initial capacity scores; **persistence analysis** determining whether capacity improvements prove durable or erode without sustained political commitment; and **shock response** studying how governance capacity responds to external events like AI accidents, public controversies, or international agreements. The UNESCO Recommendation adoption (November 2021) provides a natural quasi-experiment: comparing pre/post adoption capacity trajectories would reveal whether international frameworks causally accelerate governance development. Panel analysis requires sustained data collection effort but provides essential evidence about governance dynamics, stability, and responsiveness our static snapshot cannot address.

**5. Expanded methodological validation**: Our LLM-based scoring methodology requires validation through **human expert benchmarking at scale** to fully characterize agreement, identify systematic divergences, and optimize scoring protocols. While our current validation involves 200 policies, extending to 1,000+ policies across diverse regions, languages, and policy types would enable: **inter-rater reliability assessment** comparing LLM ensemble scores to expert consensus; **bias detection** identifying whether models systematically over/undervalue specific governance approaches, regional traditions, or policy types; **prompt engineering** testing alternative rubric presentations, scoring instructions, or few-shot examples to optimize reliability; **model comparison** evaluating whether newer models (GPT-5, Claude Opus 5, Gemini Ultra) improve scoring quality; **multilingual validation** assessing whether machine-translated and native-English policies show comparable accuracy; and **construct validation** examining whether LLM scores correlate appropriately with related constructs (democratic quality indices, regulatory effectiveness measures, institutional capacity assessments). This methodological research would strengthen confidence in LLM-based governance assessment more broadly, providing validated protocols for other researchers adopting similar approaches to measure policy quality, institutional capacity, or regulatory sophistication across large corpora.

2. **Panel analysis**: Repeat the scoring at annual intervals to track governance evolution and test causal hypotheses.

3. **Human validation**: Conduct a large-scale human coding exercise (N ≥ 200 policies) to establish ground truth against which LLM scores can be validated.

4. **Sector-specific analysis**: Disaggregate by target sector (health, finance, defence) to identify sector-specific governance patterns.

5. **Qualitative deep dives**: Case studies of frontier countries (Rwanda, Brazil, Kazakhstan) to understand the mechanisms behind over- and under-performance.
