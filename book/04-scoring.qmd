---
title: "LLM Ensemble Scoring & Validation"
---

## Measuring Governance Quality at Scale {#sec-scoring}

::: {.callout-note appearance="simple"}
**Chapter summary.** This chapter presents our LLM-based scoring methodology — a three-model ensemble that independently codes each policy on 10 dimensions. We report inter-rater reliability (ICC = 0.827, Excellent) and discuss model-specific scoring patterns.
:::

### Scoring Framework {#sec-framework}

The transition from collected documents to analyzable data required developing a comprehensive assessment framework that could systematically evaluate implementation readiness across diverse policy types, jurisdictions, and governance traditions. This framework needed to capture both the structural features that enable implementation (capacity dimensions) and the substantive ethical commitments that shape governance outcomes (ethics dimensions). Drawing on decades of implementation science and the emerging AI governance literature, we constructed a 10-dimension assessment framework organized into two complementary domains.

Each of the 2,216 policies was scored on **10 dimensions** using a 0–4 scale, where 0 indicates complete absence of the feature, 1-2 represent minimal to moderate presence, 3 indicates substantial implementation readiness, and 4 reflects comprehensive operationalization with concrete mechanisms. This five-point scale provides sufficient granularity to distinguish meaningful quality differences while maintaining inter-rater reliability — finer scales would introduce excessive noise, while coarser scales would obscure important variation.

#### Capacity Dimensions {#sec-cap-dims}

Grounded in implementation science [@mazmanian1983; @lipsky1980; @grindle1996; @fukuyama2013]:

| Code | Dimension | What It Measures |
|:---|:---|:---|
| C1 | Clarity & Specificity | Clear objectives, measurable targets, defined scope |
| C2 | Resources & Budget | Dedicated funding, staffing, infrastructure |
| C3 | Authority & Enforcement | Legal mandate, penalties, compliance mechanisms |
| C4 | Accountability & M&E | Reporting, evaluation, oversight bodies |
| C5 | Coherence & Coordination | Cross-agency alignment, international coordination |

: Capacity scoring dimensions {#tbl-cap-dims}

These five capacity dimensions operationalize the implementation conditions identified by @mazmanian1983 and extended by subsequent scholars. Clarity corresponds to Mazmanian and Sabatier's emphasis on clear objectives and causal theories; Resources captures Grindle's technical and fiscal capacity requirements; Authority reflects the legal structuring of implementation processes; Accountability operationalizes Lipsky's concern with constraining street-level discretion; and Coherence addresses the coordination challenges documented by @hjern1982. Together, they provide a comprehensive assessment of whether policies possess the institutional infrastructure necessary for execution.

#### Ethics Dimensions {#sec-eth-dims}

Grounded in AI ethics literature [@jobin2019; @floridi2018; @oecd2019; @unesco2021; @euaiact2024]:

| Code | Dimension | What It Measures |
|:---|:---|:---|
| E1 | Ethical Framework Depth | Grounding in principles, coherent ethical vision |
| E2 | Rights Protection | Privacy, non-discrimination, human oversight, transparency |
| E3 | Governance Mechanisms | Ethics boards, impact assessments, auditing |
| E4 | Operationalisation | Concrete requirements, standards, certification |
| E5 | Inclusion & Participation | Stakeholder processes, marginalised group representation |

: Ethics scoring dimensions {#tbl-eth-dims}

The ethics dimensions synthesize principles identified across the AI governance literature, particularly the convergence documented by @jobin2019 around transparency, fairness, accountability, and privacy. Framework Depth assesses whether policies ground specific requirements in coherent ethical visions rather than listing buzzwords. Rights Protection operationalizes the human-centric principles emphasized by @floridi2018 and enshrined in frameworks like UNESCO's AI Recommendation. Governance Mechanisms capture the institutional architecture for ethics oversight, while Operationalisation distinguishes aspirational statements from concrete requirements with measurable standards. Inclusion reflects the participatory governance emphasis in @oecd2019, recognizing that AI governance legitimacy depends on meaningful stakeholder engagement.

Each dimension uses explicit scoring rubrics (see @sec-appendix-rubric) with anchored examples at each scale point, ensuring that assessments rest on observable textual evidence rather than subjective impressions. Composite scores are computed as unweighted means: *Capacity* = mean(C1–C5), *Ethics* = mean(E1–E5), *Overall* = mean(all 10). This equal weighting reflects our agnostic stance on which dimensions matter most — different governance contexts may prioritize different features, and our framework captures this multidimensionality rather than imposing a single definition of quality.

### Three-Model Ensemble {#sec-ensemble}

Applying this 10-dimension framework to 2,216 documents requires a scoring approach that balances three competing demands: analytical sophistication (capturing nuanced implementation features), scale (processing millions of words of policy text), and reliability (producing consistent assessments across documents). Traditional human expert coding offers sophistication but becomes prohibitively expensive and time-consuming at this corpus size. Automated keyword-based approaches scale efficiently but lack the interpretive capacity to distinguish substantive implementation details from aspirational rhetoric. Our solution employs frontier large language models as automated policy analysts, leveraging their ability to read and interpret complex documents while maintaining consistency through ensemble design.

To mitigate single-model bias and architectural idiosyncrasies, each policy was independently scored by three frontier LLMs via the OpenRouter API, selected to represent diverse training approaches and institutional origins:

| Model | Identifier | Role | Entries Scored |
|:---|:---|:---|---:|
| Model A | Claude Sonnet 4 | Strictest scorer | 2,210 (99.7%) |
| Model B | GPT-4o | Moderate scorer | 2,216 (100%) |
| Model C | Gemini Flash 2.0 | Moderate scorer | 2,215 (100%) |

: LLM ensemble composition {#tbl-ensemble}

This ensemble design leverages complementary strengths: Claude Sonnet 4's nuanced policy interpretation and attention to implementation details, GPT-4o's balanced analytical approach and broad domain knowledge, and Gemini Flash 2.0's efficient processing and consistent scoring patterns. By combining models from three different organizations (Anthropic, OpenAI, Google) trained on potentially different corpora using different architectures, we reduce the risk that shared training biases or architectural quirks systematically skew results.

Each model received identical structured prompts containing the full policy text (up to context window limits, typically 8,000+ words) and the complete scoring rubric with anchored examples. The prompts instructed models to read the entire document, assess each dimension independently, assign a 0-4 score based on observable textual evidence, and provide brief supporting excerpts justifying each score. Models returned structured JSON-formatted outputs with dimension-level scores and evidence, enabling automated aggregation while preserving auditability through the evidence field. The final ensemble score for each dimension is the **median** of the three model scores, following the logic of robust central tendency estimation. The median approach proves superior to the mean in this context because it remains unaffected by single-model outliers and handles the systematic calibration differences we observe across models (detailed below) without requiring explicit recalibration.

The total scoring effort required **6,641 API calls** (2,216 policies × 3 models, minus a handful of failures where models returned malformed JSON or exceeded context windows). The high completion rate — 99.7% of entries successfully scored by all three models — demonstrates the robustness of the pipeline to diverse document formats and lengths.

### Inter-Rater Reliability {#sec-irr}

The validity of this entire analytical enterprise rests on a fundamental question: do the three models agree on policy quality, or do they produce idiosyncratic assessments that reflect model-specific biases rather than genuine document features? If inter-model agreement is low, the ensemble scores become arbitrary — different model combinations would yield different conclusions. If agreement is high, this provides evidence that the scores capture systematic variation in policy quality rather than measurement noise.

We assess agreement across the three LLM "raters" using multiple complementary metrics, following the framework established by @shrout1979 for inter-rater reliability in observational studies. The intraclass correlation coefficient ICC(2,1) serves as our primary reliability measure, as it appropriately handles the nested structure of our data (three models rating each policy) and quantifies the proportion of total variance attributable to true between-policy differences rather than rater disagreement. We supplement this with pairwise correlations, Fleiss' kappa for categorical agreement, and descriptive measures of score spread to provide a comprehensive reliability portrait.

#### Overall Reliability

| Metric | Value | Interpretation |
|:---|:---|:---|
| ICC(2,1) overall | **0.827** | Excellent |
| ICC(2,1) capacity | 0.824 | Excellent |
| ICC(2,1) ethics | 0.791 | Excellent |
| Mean pairwise Pearson | 0.86 | Strong |
| Mean pairwise Spearman | 0.88 | Strong |
| Mean Fleiss' κ | 0.51 | Moderate |
| Mean overall spread | 0.40/4 | Low disagreement |
| Scores within 1 point | 95.4% | High consistency |

: Inter-rater reliability summary {#tbl-irr-summary}

@tbl-irr-summary presents a remarkably consistent picture across multiple metrics. The ICC(2,1) of 0.827 indicates "Excellent" reliability under Cicchetti's (1994) guidelines (>0.75 = Excellent), meaning that approximately 83% of the variance in observed scores reflects true differences between policies rather than rater disagreement. This level of agreement is comparable to or exceeds reliability typically reported in human-coded policy analysis studies, where ICC values of 0.70-0.80 are considered strong evidence of coding quality. The high pairwise correlations (mean r = 0.86, ρ = 0.88) confirm this consistency through a different lens, while the low mean spread (0.40 points on a 4-point scale) and high within-1-point agreement (95.4%) demonstrate that models rarely produce wildly divergent assessments. Even Fleiss' kappa — a more conservative metric that treats the 0-4 scale categorically rather than continuously — achieves moderate agreement (0.51), which for a five-category scale represents substantial consensus.

Crucially, both capacity and ethics subscales achieve excellent reliability independently (ICC = 0.824 and 0.791 respectively), indicating that the strong overall agreement is not driven by a single dominant construct but reflects genuine consensus across both theoretical domains.

#### Dimension-Level ICCs

| Dimension | ICC(2,1) | Quality |
|:---|:---|:---|
| C1 Clarity | 0.720 | Good |
| C2 Resources | 0.735 | Good |
| C3 Authority | 0.751 | Excellent |
| C4 Accountability | 0.753 | Excellent |
| C5 Coherence | 0.804 | Excellent |
| E1 Framework | 0.751 | Excellent |
| E2 Rights | 0.785 | Excellent |
| E3 Governance | 0.691 | Good |
| E4 Operationalisation | 0.605 | Good |
| E5 Inclusion | 0.746 | Good |

: Dimension-level ICC values {#tbl-irr-dims}

@tbl-irr-dims reveals systematic patterns in dimension-level reliability that illuminate the scoring process. All dimensions achieve at least "Good" reliability (>0.60), with six reaching "Excellent" (>0.75). The highest agreement appears on structural features like Coherence (ICC = 0.804), Authority (0.751), and Rights Protection (0.785) — dimensions where textual evidence is relatively concrete and unambiguous. Lower (though still acceptable) reliability on Operationalisation (0.605) and Governance Mechanisms (0.691) likely reflects the greater interpretive challenge these dimensions pose: distinguishing truly operational requirements from aspirational language requires subtle judgment that even sophisticated models may approach differently. The lowest ICC (E4 Operationalisation, 0.605) still comfortably exceeds conventional acceptability thresholds (>0.40 for exploratory research, >0.60 for established scales), providing confidence that all 10 dimensions contribute meaningful signal rather than noise to the composite scores.

#### Model-Specific Scoring Patterns {#sec-model-bias}

The three models exhibit systematic scoring tendencies:

| Model | Capacity Mean | Ethics Mean | Overall Mean |
|:---|---:|---:|---:|
| A (Claude) | 0.68 | 0.46 | 0.57 |
| B (GPT-4o) | 0.92 | 0.71 | 0.81 |
| C (Gemini) | 0.93 | 0.68 | 0.81 |

: Model-level mean scores {#tbl-model-means}

@tbl-model-means exposes a striking and systematic pattern: Model A (Claude Sonnet 4) scores approximately 0.24 points lower on average than Models B and C across both capacity and ethics dimensions. This is not random noise or jurisdiction-specific bias — the pattern holds consistently across all policy types, income groups, and regions, indicating a fundamental calibration difference in how the model interprets the 0-4 scale. Model A appears to require stronger textual evidence to assign higher scores, treating the rubric descriptions more stringently than its counterparts. The gap is particularly pronounced on ethics dimensions (0.46 vs. 0.68-0.71), suggesting that Model A applies more demanding standards for what constitutes operationalized ethical governance versus aspirational principles.

Importantly, this systematic shift does not invalidate Model A's contributions to the ensemble. The high correlation between Model A's scores and those of Models B and C (r > 0.85) demonstrates that all three models agree on the *rank ordering* of policies even while disagreeing on absolute levels. The median-based aggregation proves robust to this calibration difference: it preserves the relative rankings while positioning the final scores between the strict and lenient interpretations. An alternative approach using mean scores would require explicit recalibration or standardization; the median avoids this complexity while naturally accounting for systematic shifts.

#### Agreement by Text Quality {#sec-agreement-quality}

| Text Quality | N | Mean Spread | Within 1 pt |
|:---|---:|---:|---:|
| Good (≥500 words) | 942 | 0.57 | 90.3% |
| Thin (100–499) | 805 | 0.34 | 98.9% |
| Stub (<100) | 462 | 0.13 | 99.8% |

: Agreement by text quality {#tbl-agreement-quality}

@tbl-agreement-quality reveals the expected relationship between document informativeness and scoring consensus. Models achieve near-perfect agreement on stub documents (mean spread 0.13, within-1-point agreement 99.8%), largely because these minimal texts provide insufficient evidence for any dimension to score above zero. The models converge trivially on low scores when documents offer little substance to assess. Agreement remains very high on thin documents (spread 0.34, agreement 98.9%), as these 100-499 word texts typically mention governance features without providing implementation details, again limiting the interpretive range.

The elevated disagreement on good-quality texts (spread 0.57, agreement 90.3%) should not be interpreted as a reliability failure but rather as evidence that models are engaging substantively with document content. Longer, more detailed policies present genuinely ambiguous cases where reasonable analysts might differ: Does a policy with detailed budget projections but unclear enforcement mechanisms score 2 or 3 on Resources? Does sophisticated ethical framework discussion without concrete operationalization merit a 2 or 3 on Framework Depth? These interpretive challenges produce the higher spread we observe. The fact that even for good texts, 90.3% of scores fall within 1 point indicates that disagreement occurs at boundary cases rather than reflecting fundamental divergence in assessment.

### Composite Scores {#sec-composite-scores}

The resulting ensemble produces composite scores with the following distributions:

| Component | Mean | SD | Median | IQR |
|:---|---:|---:|---:|:---|
| Capacity (C1–C5) | 0.83 | 0.77 | 0.60 | 0.00–1.40 |
| Ethics (E1–E5) | 0.61 | 0.62 | 0.40 | 0.00–1.00 |
| Overall (all 10) | 0.73 | 0.66 | 0.50 | 0.10–1.15 |

: Composite score distributions {#tbl-composites}

@tbl-composites summarizes the final ensemble scores that serve as the primary data for all subsequent analyses. Three distributional features prove particularly consequential for analytical choices in later chapters.

First, the **strong floor effect** — with 27.6% of policies scoring exactly zero on capacity and 36.3% on ethics — indicates that more than a quarter of documents in the OECD.AI Observatory contain insufficient implementation detail to score above the minimum threshold on our framework. These zeros are not missing data but substantive findings: many AI governance documents consist of brief announcements, aspirational statements, or high-level principles without operational content. This censoring at zero violates the assumptions of standard OLS regression, motivating the Tobit models we employ in @sec-cap-determinants to correct for attenuation bias.

Second, the **right skew** in all three distributions — with medians substantially below means and interquartile ranges concentrated in the lower half of the scale — reveals that most policies cluster at the low end of implementation readiness, while a smaller set of comprehensive policies achieve substantially higher scores. This heterogeneity suggests that focusing solely on mean comparisons would obscure important distributional differences, motivating the quantile regression approach that examines effects at different points of the score distribution.

Third, the systematic **capacity-ethics gap** — with policies averaging 0.83 on implementation architecture but only 0.61 on ethics operationalization — points to a prioritization pattern: governments more frequently specify institutional structures, budgets, and authorities than operationalize ethical principles through concrete requirements. This gap receives detailed examination in @sec-pca-nexus, where we explore the capacity-ethics nexus and identify distinct governance typologies.

### Validation Discussion {#sec-validation-discussion}

The use of large language models as automated policy coders represents a methodological innovation with both promise and peril. Our approach builds on a growing body of evidence demonstrating that frontier language models can perform complex text annotation tasks at or above human-coder quality [@gilardi2023; @tornberg2024]. Recent validation studies show that LLMs achieve reliability comparable to trained human coders on tasks ranging from sentiment classification to ideological scaling, while processing text orders of magnitude faster and at far lower cost. However, these findings come with important caveats [@pangakis2023]: LLM performance varies substantially across task types, prompt formulations, and model versions, and models can exhibit systematic biases learned from training data that may not align with human expert judgment on normatively contentious dimensions.

Three features of our methodological design directly address these validity concerns. The **multi-model ensemble** reduces the risk that findings reflect idiosyncrasies of any single model's training data or architectural choices by combining three independently-developed models from different organizations. If all three models converge on similar assessments despite their different origins, this provides stronger evidence of validity than relying on a single model's output. The **structured output with evidence** requirement — where models must provide supporting textual excerpts justifying each score — enables post-hoc auditing and increases the probability that models ground assessments in observable document features rather than generating plausible-sounding scores without textual basis. The **median aggregation** strategy proves robust both to single-model outliers and to the systematic calibration difference we observe across models, avoiding the need for explicit recalibration while preserving relative rankings.

Important limitations remain that readers should bear in mind when interpreting results. The three models, despite their different origins, may share biases inherited from overlapping training corpora — particularly given that all were likely exposed to prominent AI governance documents like the OECD AI Principles and EU AI Act during training. The scoring rubric itself, while grounded in implementation science theory and AI governance scholarship, necessarily involves subjective judgments about what constitutes "adequate" clarity or "substantial" resource allocation — dimensions on which even expert human coders would reasonably disagree. Our ensemble treats all three models as equally authoritative through median aggregation, but this may not reflect their actual relative validity — it is conceivable that one model's systematic stringency or leniency better aligns with ground truth than the ensemble median, though we lack a gold standard against which to evaluate this.

These methodological uncertainties motivate the extensive robustness checks presented in @sec-robustness, where we examine whether core findings hold across alternative specifications, subsamples, and aggregation methods. The consistency of results across these checks provides additional confidence that our conclusions reflect genuine patterns in policy quality rather than artifacts of measurement choices.
