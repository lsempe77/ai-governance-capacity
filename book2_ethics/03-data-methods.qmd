---
title: "Data & Methods"
---

## The OECD.AI Corpus {#sec-data-methods}

::: {.callout-note appearance="simple"}
**Section summary.** This section describes the data collection pipeline: from the OECD.AI Policy Observatory through document retrieval, text extraction, and quality classification. We detail the construction of a 2,216-policy corpus with 11.4 million words of analysis-ready text across 70+ jurisdictions.
:::

### Corpus Construction {#sec-data-source}

The data derive from the **OECD.AI Policy Observatory** [@oecdai2024], the most comprehensive international tracker of AI policy initiatives. Established as a collaborative effort among OECD member states and partner countries, the Observatory serves as the global standard for monitoring AI governance activity. It catalogues government actions related to AI — including national strategies, legislation, executive orders, guidelines, and programmes — with structured metadata on jurisdiction, year, policy type, target sectors, and responsible organisations. This structured approach makes the Observatory uniquely suited for systematic cross-national comparison, as each entry follows a consistent documentation schema that enables quantitative analysis at scale.

The complete Observatory was systematically scraped as of January 2026, obtaining **2,216 policy entries** spanning **70+ jurisdictions** and the years **2017–2025**. This snapshot represents the state of global AI governance at a critical juncture, as many jurisdictions transition from voluntary guidelines to binding regulation.

| Metric | Value |
|:---|---:|
| Total policy entries | 2,216 |
| Unique jurisdictions | 70+ |
| Time span | 2017–2025 |
| Policy types | Strategies, laws, guidelines, executive orders, programmes |
| Source | OECD.AI Policy Observatory |

: Corpus overview {#tbl-corpus-overview}

@tbl-corpus-overview shows the breadth of our corpus, which encompasses nearly every documented AI governance initiative globally over the past eight years. The 70+ jurisdictions include not only major economies but also developing countries in Africa, Asia, and Latin America, providing the geographic diversity necessary to examine capacity gaps across income levels.

**Document Retrieval.** The Observatory provides brief descriptions and links to source documents but does not host full texts. To enable detailed assessment, a five-strategy cascading retrieval pipeline was constructed:

1. Direct download from `source_url` (~60% success)
2. Scraping embedded links from each OECD.AI entry page
3. Internet Archive Wayback Machine for moved/expired URLs
4. DuckDuckGo search with policy title, jurisdiction, and file type restrictions
5. Claude API web search for the most difficult cases

This pipeline achieved ~94% coverage (2,085 documents retrieved). The remaining entries, mostly press releases or brief announcements, were retained as OECD snippets.

**Text Extraction.** Policy documents arrive in varied formats: text-based or scanned PDFs, web pages with complex navigation, documents ranging from single pages to hundred-page legislative texts. Format-specific extraction tools were employed: PyMuPDF (`fitz`) for PDFs, `trafilatura` for HTML content extraction, and OECD snippet text as a fallback.

Each document was classified into three quality tiers by word count:

| Quality Tier | Word Count | N | % | Description |
|:---|:---|---:|---:|:---|
| Good | ≥500 words | 948 | 42.8% | Full analysis possible |
| Thin | 100–499 words | 806 | 36.4% | Usable with caveats |
| Stub | <100 words | 462 | 20.8% | Minimal text only |
| | **Analysis-ready** | **1,754** | **79.2%** | Good + Thin |

: Text quality distribution {#tbl-text-quality}

@tbl-text-quality shows ~80% of the corpus (1,754 documents) contains sufficient text for analysis, with 43% "Good" quality (500+ words). The 806 "Thin" documents (100-499 words) support basic scoring but may lack detail. The 462 "Stub" entries (<100 words) contribute little analytically. Total corpus: 11.4 million words; median document length: 1,247 words.

### Sample and Metadata {#sec-enriched-corpus}

**Enriched Corpus.** The pipeline produced `corpus_enriched.json`, merging OECD metadata with extracted content and quality assessments. Each entry preserves original metadata (title, jurisdiction, year, URL, policy type, sectors) plus extracted text, quality classification, word count, and extraction method.

Jurisdictions were mapped to World Bank classifications: income groups (HI, UMI, LMI, LI) and regions (EAP, ECA, LAC, MENA, NAM, SA, SSA). For North-South analyses, we use a binary HI vs. Developing classification. GDP per capita (current USD, 2023) enables continuous wealth comparisons. International organizations were flagged and excluded from country-level analyses.

**Sample Composition.** The final analytical sample reflects the OECD.AI Observatory's coverage, which skews toward high-income countries:

| Income Group | N Policies | % | N Countries |
|:---|---:|---:|---:|
| High Income | 1,700 | 76.7% | ~40 |
| Developing | 397 | 17.9% | ~30 |
| International | 119 | 5.4% | — |
| **Total** | **2,216** | **100%** | **70+** |

: Sample by income group {#tbl-sample-income}

@tbl-sample-income shows compositional imbalance: high-income countries account for 77% of policies, developing countries 18%. This reflects genuine AI governance activity distribution — high-income countries produce more policies and maintain more accessible archives. Robustness checks in @sec-robustness address this imbalance.

### Analytical Pipeline Overview {#sec-pipeline-overview}

@fig-pipeline shows the transformation from 2,216 entries through retrieval, extraction, scoring, and analysis to 120 outputs.

```{mermaid}
%%| label: fig-pipeline
%%| fig-cap: "Analytical pipeline from corpus to results"
%%| fig-width: 8

graph LR
    A[OECD.AI<br/>2,216 entries] --> B[Document<br/>Retrieval<br/>94% coverage]
    B --> C[Text<br/>Extraction<br/>1,754 ready]
    C --> D[LLM Scoring<br/>3-model ensemble<br/>6,641 calls]
    D --> E[20 Analyses<br/>120 outputs]
    
    style A fill:#e1f5fe
    style B fill:#e8f5e9
    style C fill:#fff3e0
    style D fill:#fce4ec
    style E fill:#f3e5f5
```

@fig-pipeline shows how each stage transforms the data: from initial policy entries through document retrieval and text extraction (the data collection phase documented in preceding sections), to LLM-based scoring (detailed in @sec-scoring), culminating in the 20 analytical chapters that follow. The 6,641 LLM API calls represent three model assessments for each of the 2,216 policies across 10 dimensions, with the ensemble approach ensuring reliability through inter-model agreement.

### Analytical Methods {#sec-analytical-methods}

The statistical analyses in subsequent chapters employ multiple complementary methods. This methodological pluralism enables robust inference: findings that emerge consistently across diverse analytical approaches inspire greater confidence. Here we overview the core techniques; specific model specifications appear in their respective chapters.

**Text-to-data conversion.** The foundational step is converting unstructured policy documents into structured quantitative scores. Each LLM reads the full policy document, applies the detailed scoring rubric for all 10 dimensions simultaneously, and returns structured JSON-formatted scores with textual evidence. The three-model ensemble (Claude Sonnet 4, GPT-4o, Gemini Flash 2.0) uses the median score as the final assessment. ICC(2,1) = 0.827 demonstrates excellent inter-rater reliability, comparable to or exceeding typical human coder agreement. Detailed validation appears in @sec-scoring.

**Descriptive analysis.** Each analytical section begins with descriptive statistics and visual exploration: dimension-specific histograms, ridge plots across groups, radar charts, box-and-violin overlays, and heatmaps.

**Regression models.** Chapters examining determinants employ four complementary approaches: standard OLS to establish baseline relationships; multilevel models with random intercepts for countries to account for nested structure; quantile regression to examine heterogeneous effects across the distribution; and Tobit models to address the substantial floor effect (27.6% of policies score exactly zero) through left-censoring.

**Inequality analysis.** Gini coefficients and Lorenz curves quantify overall inequality. Theil's T index enables exact additive decomposition into between-group and within-group components. Policy portfolio analysis examines breadth versus depth.

**Temporal analysis.** Panel data methods separate within-country trends from between-country differences. First-difference models remove country fixed effects; Cohen's d effect sizes assess substantive significance; convergence analysis tests whether income-group gaps are narrowing, widening, or stable.

**Multivariate methods.** PCA examines latent structure underlying the 10 governance dimensions. Cronbach's alpha assesses internal consistency. K-means clustering identifies natural policy groupings, with optimal k determined through silhouette coefficients and bootstrap stability analysis.

**Hypothesis testing.** Welch's t-tests and Mann-Whitney U tests for group comparisons, chi-square tests for categorical associations, with exact p-values, effect sizes (Cohen's d, Cramér's V), and confidence intervals throughout.

### Limitations and Reproducibility {#sec-corpus-limitations}

The OECD.AI Observatory, while the most comprehensive international tracker available, introduces several systematic biases.

**English-language dominance.** The Observatory's documentation practices favour English-language sources. Policies originally published in English tend to receive fuller descriptions, more accessible source links, and more detailed metadata. The LLM scoring models, while multilingual, perform best on English text. This creates a measurement pathway from language of publication → text quality → ethics score that may systematically disadvantage non-Anglophone jurisdictions.

**OECD member-state reporting bias.** OECD member countries have institutional incentives and administrative capacity to report their policy activities. Non-member countries may have governance instruments that do not appear in the database. The 77% high-income composition (@tbl-sample-income) likely reflects this reporting asymmetry as much as the actual distribution of AI governance activity.

**Sub-national exclusion.** The Observatory focuses on national-level policies, largely excluding state, provincial, and municipal AI governance. In federal systems—the United States, India, Brazil, Germany—substantial governance activity occurs at sub-national levels. This exclusion may systematically understate governance capacity in federal countries.

**Temporal coverage unevenness.** Earlier years (2017–2019) contain fewer entries, particularly from developing countries that adopted AI governance later. Temporal analyses should be interpreted cautiously for this period.

**Classification quality.** The Observatory's own categorisation of policy types, target sectors, and responsible organisations introduces noise. These classification decisions, made by OECD analysts, propagate through all subsequent analyses.

These limitations counsel caution when interpreting apparent gaps, particularly between high-income and developing nations. The robustness checks in @sec-robustness address the most consequential of these biases directly.

**Reproducibility.** All code is available at <https://github.com/lsempe77/ai-governance-capacity>. The pipeline uses deterministic document IDs (`MD5(url)[:12]`) to ensure reproducibility of the corpus-to-analysis link. API calls to LLM providers used fixed model identifiers and structured JSON output schemas.

**Use of Large Language Models.** This research employs large language models in two distinct capacities, both disclosed here in the interest of methodological transparency.

*For data analysis:* Large language models (Claude Sonnet 4, GPT-4o, and Gemini Flash 2.0) serve as the core analytical instrument, functioning as automated policy coders that convert unstructured policy documents into structured quantitative scores. This use constitutes the research methodology itself and is documented extensively throughout @sec-data-methods and @sec-scoring, including validation against human expert ratings. All LLM-generated scores are preserved in the public repository, enabling verification and replication of our analytical pipeline.

*For writing assistance:* Large language models (primarily GitHub Copilot and Claude) provided assistance with text editing during manuscript preparation. All LLM-generated text was reviewed, revised, and approved by the author, who takes full responsibility for the accuracy and integrity of the final content. LLMs did not generate substantive intellectual contributions, interpret findings, or make analytical decisions—these remained under direct human control throughout the research process.

This dual disclosure reflects a commitment to transparency in an era where LLM use in research is becoming common. We distinguish between LLMs as research instruments (where their use is the methodology being validated) and LLMs as writing assistants (where they augment but do not replace human scholarly judgment).
