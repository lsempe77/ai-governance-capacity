---
title: "Data and Methods"
---

## The OECD.AI Corpus {#sec-data-methods}

This study compiled 2,216 AI policies from the OECD.AI Policy Observatory, retrieved the original documents (94% success rate), and extracted 11.4 million words of analysis-ready text.

### Corpus Construction {#sec-data-source}

The **OECD.AI Policy Observatory** [@oecdai2024] is the most comprehensive international tracker of AI policy initiatives. It catalogues government actions—national strategies, legislation, executive orders, guidelines, programmes—with structured metadata on jurisdiction, year, policy type, and target sectors. We scraped the complete Observatory as of January 2026, obtaining **2,216 policy entries** spanning **70+ jurisdictions** from **2017–2025**.

| Metric | Value |
|:---|---:|
| Total policy entries | 2,216 |
| Unique jurisdictions | 70+ |
| Time span | 2017–2025 |
| Policy types | Strategies, laws, guidelines, executive orders, programmes |
| Source | OECD.AI Policy Observatory |

: Corpus overview {#tbl-corpus-overview}

The 70+ jurisdictions include not only major economies but also developing countries across Africa, Asia, and Latin America—the geographic diversity needed to examine capacity gaps across income levels.

**Document Retrieval.** The Observatory points to documents but does not host them. Original URLs expire, documents are renamed, and websites restructure. The study employed a five-strategy retrieval pipeline: direct download from the source URL (~60% success), scraping embedded links from OECD.AI pages, querying the Internet Archive Wayback Machine for expired URLs, targeted DuckDuckGo searches, and Claude API web search for difficult cases.

This achieved ~94% coverage: 2,085 documents retrieved. The remainder—mostly press releases and brief announcements—remained available as OECD snippets.

**Text Extraction.** PDFs, HTML pages, scanned documents—each format required specialized handling. The study used PyMuPDF for text-based PDFs, `trafilatura` for web pages (stripping navigation and boilerplate), and OECD snippets where no downloadable source existed.

Documents were classified by word count:

| Quality Tier | Word Count | N | % | Description |
|:---|:---|---:|---:|:---|
| Good | ≥500 words | 948 | 42.8% | Full analysis possible |
| Thin | 100–499 words | 806 | 36.4% | Usable with caveats |
| Stub | <100 words | 462 | 20.8% | Minimal text only |
| | **Analysis-ready** | **1,754** | **79.2%** | Good + Thin |

: Text quality distribution {#tbl-text-quality}

Nearly 80% of the corpus contains sufficient text for reliable analysis. The full corpus totals 11.4 million words, with a median document length of 1,247 words.

### Enriched Corpus and Sample Composition {#sec-enriched-corpus}

The pipeline produced `corpus_enriched.json`, merging OECD metadata with extracted text and quality assessments. Each jurisdiction was mapped to World Bank classifications: income groups (HI, UMI, LMI, LI), regions (EAP, ECA, LAC, MENA, NAM, SA, SSA), and GDP per capita (current USD, 2023). For North–South analyses, a binary HI vs. Developing classification was used. International organisations were flagged and excluded from country-level analyses.

The final analytical sample reflects the OECD.AI Observatory's coverage, which skews toward high-income countries:

| Income Group | N Policies | % | N Countries |
|:---|---:|---:|---:|
| High Income | 1,700 | 76.7% | ~40 |
| Developing | 397 | 17.9% | ~30 |
| International | 119 | 5.4% | — |
| **Total** | **2,216** | **100%** | **70+** |

: Sample by income group {#tbl-sample-income}

@tbl-sample-income shows a substantial compositional imbalance: high-income countries account for 77% of policies in the corpus, while developing countries contribute only 18%. This disparity reflects the genuine distribution of AI governance activity globally: high-income countries have produced more policies, published more documentation, and maintained more accessible policy archives. However, this imbalance creates analytical challenges, as conventional statistical comparisons assume relatively balanced groups. We address potential selection effects and the implications of unbalanced samples through comprehensive robustness checks in @sec-robustness, including analyses restricted to well-documented policies and country-level aggregations that equalize representation.

### Analytical Pipeline Overview {#sec-pipeline-overview}

The journey from raw OECD.AI metadata to empirical findings involves multiple transformation stages, each addressing distinct methodological challenges. @fig-pipeline visualizes this progression, showing how 2,216 initial entries flow through retrieval, extraction, scoring, and analysis to produce the 120 outputs (figures, tables, statistical tests) that appear in subsequent chapters. This pipeline architecture separates data collection concerns from analytical decisions, enabling transparent documentation of how each methodological choice affects downstream results.

```{mermaid}
%%| label: fig-pipeline
%%| fig-cap: "Analytical pipeline from corpus to results"
%%| fig-width: 8

graph LR
    A[OECD.AI<br/>2,216 entries] --> B[Document<br/>Retrieval<br/>94% coverage]
    B --> C[Text<br/>Extraction<br/>1,754 ready]
    C --> D[LLM Scoring<br/>3-model ensemble<br/>6,641 calls]
    D --> E[20 Analyses<br/>120 outputs]
    
    style A fill:#e1f5fe
    style B fill:#e8f5e9
    style C fill:#fff3e0
    style D fill:#fce4ec
    style E fill:#f3e5f5
```

### Analytical Methods {#sec-analytical-methods}

The statistical analyses in subsequent chapters employ multiple complementary methods to examine governance capacity from different angles. This methodological pluralism enables robust inference: findings that emerge consistently across diverse analytical approaches inspire greater confidence than those dependent on a single modeling choice. Here we overview the core analytical techniques; specific model specifications appear in their respective chapters.

**Text-to-data conversion** used frontier LLMs as automated policy analysts. Each model reads the full document (up to 8,000+ words), applies our scoring rubric, and returns structured JSON with scores and supporting evidence. This preserves the interpretive sophistication of human expert coding while achieving the scale needed for 2,216 documents. The three-model ensemble (Claude Sonnet 4, GPT-4o, Gemini Flash 2.0) functions as a panel of expert raters, with the median score serving as the final assessment. The resulting ICC(2,1) = 0.827 demonstrates excellent reliability—comparable to or exceeding typical human coder agreement on complex policy dimensions. Detailed validation appears in @sec-scoring.

**Descriptive analysis.** Each analytical section begins with descriptive statistics and visual exploration: histograms with frequency annotations, ridge plots showing density distributions across groups, radar charts for multidimensional profiles, and heatmaps for clustering patterns. These visualizations uncover patterns that summary statistics alone would obscure.

**Regression models.** Chapters examining determinants employ four approaches: OLS regression for baseline relationships, multilevel models with random intercepts for countries (correcting for nested structure), quantile regression for heterogeneous effects across the distribution, and Tobit models addressing the substantial floor effect (27.6% of policies score exactly zero).

**Inequality analysis.** Inequality chapters use decomposition techniques: Gini coefficients and Lorenz curves for overall inequality, Theil's T index for between-group versus within-group decomposition, and policy portfolio analysis distinguishing coverage gaps from implementation quality.

**Temporal analysis.** Chapters examining dynamics over time use panel data methods: first-difference models for year-to-year changes, Cohen's d effect sizes for substantive significance, and convergence analysis testing whether income-group gaps are narrowing, widening, or stable.

**Multivariate methods.** PCA examines the latent structure underlying governance dimensions, with optimal components determined by the Kaiser criterion (eigenvalues > 1). Cronbach's alpha assesses internal consistency. K-means clustering identifies natural policy groupings, with optimal k determined through silhouette coefficients.

**Hypothesis testing.** We use Welch's t-tests for mean comparisons (avoiding equal variance assumptions), Mann-Whitney U tests when distributions violate normality, and chi-square tests for categorical outcomes. We report exact p-values, effect sizes (Cohen's d, Cramér's V), and confidence intervals throughout.

### Limitations and Reproducibility {#sec-corpus-limitations}

The OECD.AI Observatory, while the most comprehensive international tracker available, introduces several systematic biases relevant to UNESCO alignment measurement.

**English-language dominance.** The Observatory favours English-language sources. Policies published in English receive fuller documentation, and the LLM scoring models perform best on English text. Countries that adopted the UNESCO Recommendation and developed national responses in French, Arabic, or other languages may have their alignment understated if only abbreviated English summaries are available.

**OECD member-state reporting bias.** OECD member countries have institutional incentives to report policy activities. Non-member countries—including many UNESCO member states that adopted the Recommendation—may have governance instruments that do not appear in the database. The 77% high-income composition of the corpus likely overstates wealthy-country governance activity relative to the global picture.

**Sub-national exclusion.** The Observatory focuses on national-level policies, excluding state, provincial, and municipal AI governance. In federal systems, substantial governance activity occurs at sub-national levels and may reflect UNESCO alignment not captured here.

**Temporal coverage unevenness.** The post-2021 period is critical for assessing UNESCO influence, but later years may have incomplete coverage if the Observatory's cataloguing lags behind policy adoption. This could bias temporal analyses.

**UNESCO-specific limitation.** The Observatory was not designed to track UNESCO alignment specifically. Policies that explicitly reference and implement the UNESCO Recommendation may not be tagged differently from policies developed independently. The alignment measurement thus captures *substantive overlap* with UNESCO components rather than *intentional implementation* of the Recommendation.

These limitations counsel caution, particularly when interpreting the pre/post 2021 comparison. The robustness checks in @sec-robustness address the most consequential biases directly.

**Reproducibility.** All code is available at <https://github.com/lsempe77/ai-governance-capacity>. The pipeline uses deterministic document IDs (`MD5(url)[:12]`) to ensure reproducibility of the corpus-to-analysis link. API calls to LLM providers used fixed model identifiers and structured JSON output schemas.

**Use of Large Language Models.** This research employs large language models in two distinct capacities, both of which we disclose here in the interest of methodological transparency.

**For data analysis:** Large language models (Claude Sonnet 4, GPT-4o, and Gemini Flash 2.0) serve as the core analytical instrument, functioning as automated policy coders that convert unstructured policy documents into structured quantitative scores. This use constitutes the research methodology itself and is documented extensively throughout @sec-data-methods and @sec-scoring, including validation against human expert ratings. All LLM-generated scores are preserved in the public repository, enabling verification and replication of our analytical pipeline.

**For writing assistance:** Large language models (primarily GitHub Copilot and Claude) provided assistance with text editing during manuscript preparation. All LLM-generated text was reviewed, revised, and approved by the author, who takes full responsibility for the accuracy and integrity of the final content. LLMs did not generate substantive intellectual contributions, interpret findings, or make analytical decisions; these remained under direct human control throughout the research process.

This dual disclosure reflects our commitment to transparency in an era where LLM use in research is becoming common. We distinguish between LLMs as research instruments (where their use is the methodology being validated) and LLMs as writing assistants (where they augment but do not replace human scholarly judgment).
