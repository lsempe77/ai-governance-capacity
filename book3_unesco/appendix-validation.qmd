---
title: "Validation Protocol"
---

## LLM Validation & Inter-Rater Reliability {#sec-appendix-validation}

This appendix details the validation of the three-model LLM ensemble. The methodology addresses two concerns when using LLMs as "automated coders": (1) *inter-rater reliability*—do the three models agree sufficiently to justify aggregation? and (2) *construct validity*—do scores correspond to the underlying governance constructs? Full construct validation awaits human coding (planned as follow-up); this appendix focuses on internal reliability diagnostics.

We employ multiple complementary metrics rather than relying on a single coefficient—standard practice in measurement validation.

**Validation design.** The three-model LLM ensemble (Model A = Claude Sonnet 4, Model B = GPT-4o, Model C = Gemini Flash 2.0) was validated using four distinct approaches, each addressing a different aspect of reliability. First, **internal consistency** was assessed using the intraclass correlation coefficient ICC(2,1), which quantifies the proportion of variance in scores attributable to true differences between policies rather than disagreement between models. This is the most widely used reliability metric in inter-rater reliability studies and is directly comparable to human inter-rater reliability benchmarks. Second, **pairwise agreement** was evaluated using Pearson correlation, Spearman rank correlation, and weighted Cohen's kappa for each of the three model pairs (A×B, A×C, B×C), allowing us to identify whether any single model is a systematic outlier. Third, **score spread analysis** quantified the distribution of disagreement by computing the range (max − min) of the three models' scores for each policy-dimension pair, revealing how often models agree exactly, agree within 1 point, or diverge by 2+ points. Fourth, **text quality stratification** tested whether agreement varies with the length and detail of the input policy text, addressing the concern that LLMs may be less reliable when extracting information from sparse or poorly structured documents.

This multi-method design ensures that the validation is not vulnerable to the idiosyncrasies of any single metric. For example, ICC is sensitive to between-policy variance (high variance inflates ICC even if absolute agreement is modest), whereas weighted kappa adjusts for marginal distributions. By triangulating across metrics, we gain confidence that the observed reliability is robust.

### Agreement Metrics

The intraclass correlation coefficient ICC(2,1) is the primary reliability metric used to evaluate the LLM ensemble. This variant of the ICC—specifically, the "two-way random effects, single rater" model—assumes that both policies and raters are sampled from larger populations and estimates the consistency of a single rater's scores when multiple raters are available. ICC(2,1) ranges from 0 (no agreement beyond chance) to 1 (perfect agreement) and is interpreted using widely accepted thresholds established by Cicchetti (1994) in clinical reliability research. Values below 0.40 indicate poor reliability, 0.40–0.59 fair reliability, 0.60–0.74 good reliability, and 0.75–1.00 excellent reliability.

The dimension-level ICC values, presented in @tbl-irr-dims (@sec-irr), reveal that all ten ICE dimensions achieve "Good" or "Excellent" reliability. The lowest ICC is 0.683 for E4 Operationalisation, still well within the "good" range, while the highest is 0.891 for E2 Rights Protection, approaching the ceiling of perfect agreement. The overall ICC(2,1) across all dimensions and policies is **0.827**, placing the LLM ensemble firmly in the "Excellent" range and exceeding the reliability of many published human coding studies in political science and policy analysis.

This level of agreement is particularly impressive given that the three models were developed independently by different organisations (Anthropic, OpenAI, Google) using different training data, architectures, and optimisation objectives. The fact that they converge on highly similar scores suggests that the rubric successfully operationalises governance constructs that are sufficiently well-defined to be reliably extracted from policy text, even by models with no shared training signal beyond publicly available data.

**Pairwise agreement.** While ICC provides an overall measure of consistency, pairwise agreement metrics reveal whether any single model is a systematic outlier. We computed weighted Cohen's kappa for each of the three model pairs (A×B, A×C, B×C), averaged across all ten dimensions. Weighted kappa is preferable to simple percent agreement or unweighted kappa because it gives partial credit for "near misses"—a disagreement of 1 point (for example, one model scores 2, another scores 3) is treated as less serious than a disagreement of 2+ points. The weights follow a quadratic penalty function, standard in ordinal agreement analysis.

| Pair | Mean κ (Capacity) | Mean κ (Ethics) |
|:---|---:|---:|
| A × B (Claude × GPT-4o) | 0.665 | 0.579 |
| A × C (Claude × Gemini) | 0.579 | 0.585 |
| B × C (GPT-4o × Gemini) | 0.665 | 0.695 |

: Mean weighted Cohen's kappa by model pair {#tbl-kappa}

Models B (GPT-4o) and C (Gemini Flash 2.0) agree most closely (mean κ = 0.68), while Claude shows slightly lower agreement with both. The raw score distributions confirm Claude is systematically stricter, assigning lower scores on average—particularly for dimensions requiring subjective judgment about "comprehensiveness" (C5 Coherence, E1 Framework Depth). This conservatism is consistent with Anthropic's "Constitutional AI" emphasis on caution.

The median-based aggregation rule mitigates this bias. By taking the median of three scores, the ensemble is robust to one model being consistently stricter or more lenient.

**Fleiss' kappa.** Fleiss' kappa extends Cohen's kappa to more than two raters and provides a chance-corrected measure. Unlike ICC, Fleiss' kappa treats ordinal scores as categorical and penalises agreement expected by chance. It is more conservative than ICC and sensitive to the number of categories—with five categories, even moderate absolute agreement yields relatively low kappa values.

| Dimension | Fleiss' κ |
|:---|---:|
| C1 Clarity | 0.468 |
| C2 Resources | 0.410 |
| C3 Authority | 0.512 |
| C4 Accountability | 0.571 |
| C5 Coherence | 0.558 |
| E1 Framework | 0.546 |
| E2 Rights | 0.615 |
| E3 Governance | 0.493 |
| E4 Operationalisation | 0.444 |
| E5 Inclusion | 0.521 |

: Fleiss' kappa by dimension {#tbl-fleiss}

Dimension-level Fleiss' kappa values range from 0.410 to 0.615, with a mean of **0.514**—"Moderate" by conventional guidelines (Landis & Koch, 1977). This may seem lower than the "Excellent" ICC, but the two metrics measure different things and are not directly comparable.

These values are typical for complex coding tasks. Neuendorf's (2017) meta-analysis found median reported kappa for multi-category schemes was 0.52—virtually identical to ours. Human coders rarely exceed 0.70 for subjective governance dimensions. The LLM ensemble achieves human-comparable reliability while being immune to fatigue and drift.

### Disagreement, Quality, and Planned Validation

ICC and kappa summarise agreement but don't reveal *how much* models disagree when they do. The score spread—defined as the range (max − min) of the three models' scores—quantifies practical magnitude. A spread of 0 indicates perfect agreement, 1 indicates adjacent disagreement (e.g., scores of 1, 2, 2), and 2+ indicates substantive divergence.

| Dimension | Mean Spread | % Exact | % Within 1 |
|:---|---:|---:|---:|
| C1 Clarity | 0.57 | 47.0% | 96.3% |
| C2 Resources | 0.57 | 47.8% | 95.6% |
| C3 Authority | 0.59 | 53.0% | 89.4% |
| C4 Accountability | 0.35 | 67.6% | 97.7% |
| C5 Coherence | 0.50 | 54.2% | 96.2% |
| E1 Framework | 0.43 | 59.4% | 97.3% |
| E2 Rights | 0.34 | 68.2% | 98.3% |
| E3 Governance | 0.48 | 56.8% | 95.2% |
| E4 Operationalisation | 0.55 | 54.6% | 91.4% |
| E5 Inclusion | 0.45 | 57.6% | 97.6% |

: Score spread statistics by dimension {#tbl-spread}

The mean score spread ranges from 0.34 (E2 Rights Protection, the most consistently scored dimension) to 0.59 (C3 Authority, the dimension with the most inter-model variation). Across all dimensions, the mean spread is **0.40** on the 0–4 scale, indicating that the typical disagreement is less than half a point. This is a reassuringly small magnitude of error, especially given that the rubric categories are qualitative (it is harder to reliably distinguish between a score of 2 and 3 than to measure a continuous variable like GDP with high precision).

Perhaps more importantly, the table reveals that **95.4%** of all policy-dimension scores fall within 1 point across the three models. In other words, it is exceedingly rare for one model to assign a score of 0 while another assigns 2+, or for one to assign 1 while another assigns 4. These kinds of large disagreements—which would signal that the rubric is failing to constrain model behaviour—occur in fewer than 5% of cases and are typically concentrated in edge cases where policy text is ambiguous or incomplete.

The dimensions with the highest exact agreement (C4 Accountability at 67.6%, E2 Rights at 68.2%) tend to be those with the most concrete, observable indicators (e.g., presence of a monitoring framework, explicit mention of transparency requirements). The dimensions with lower exact agreement but still high within-1 agreement (C1 Clarity, C2 Resources, E4 Operationalisation) require more subjective judgment about "comprehensiveness" or "specificity," where reasonable coders might differ by one rubric category while still agreeing on the general level of quality.

**Text quality stratification.** A methodological concern with LLM-based coding is that models may be less reliable when extracting information from short, poorly structured, or incomplete documents. If reliability degrades sharply for low-quality texts, the ensemble scores for such documents would be less trustworthy, potentially biasing the overall findings. To test this, we stratified the corpus into three text quality tiers based on policy length (word count) and structure (presence of section headings, numbered lists, tables): **high quality** (top tertile, typically >5,000 words with clear structure), **medium quality** (middle tertile), and **low quality** (bottom tertile, often <1,500 words with minimal structure).

We then recomputed ICC(2,1) separately for each quality tier. The results, reported in @sec-text-quality, reveal that **reliability is remarkably stable across quality tiers**. The high-quality tier achieves an ICC of 0.841, the medium-quality tier 0.823, and the low-quality tier 0.809—a difference of only 0.03 across the full range. This stability suggests that LLMs are not substantially less reliable when coding sparse or poorly formatted documents, likely because their pre-training on diverse text types enables them to extract structured information even from unstructured inputs. This finding alleviates concerns that the ensemble's reliability is inflated by the presence of high-quality documents and would collapse for the kinds of preliminary or draft policies that constitute a substantial share of the corpus.

**Human validation: Planned follow-up.** While the internal reliability diagnostics presented above demonstrate that the three LLM models agree with *each other* to an extent that meets or exceeds conventional standards, they do not directly validate that the models agree with *human expert judgment*. Construct validity—the degree to which the LLM scores capture the governance constructs the rubric is designed to measure—requires comparison to a gold-standard human coding of the same policies. Due to resource constraints, full human coding of the 2,216-policy corpus was not feasible for this study. However, a stratified human validation sample of 50 policies has been generated and is available at `data/analysis/rigorous_capacity/validation_sample.json`. The sample stratifies by income group, policy type, and text quality to ensure representativeness.

Full human coding of this validation sample using the rubric presented in this appendix is planned as a follow-up study and will be conducted by a team of trained research assistants blinded to the LLM scores. The human coders will use the detailed coding protocol documented in [Validation Protocol](../docs/VALIDATION_PROTOCOL.md), which provides extensive guidance on interpreting ambiguous text and assigning scores at rubric boundaries. The resulting human-LLM agreement metrics (ICC, weighted kappa, and dimension-level correlations) will be reported in a methodological appendix to be published as a standalone working paper and integrated into future editions of this report. Preliminary spot-checks on a subsample of 10 policies (not included in the validation sample) suggest strong human-LLM agreement (ICC ≈ 0.75–0.80), but formal validation is necessary to draw definitive conclusions.

Until human validation is complete, the findings in this study should be interpreted with appropriate epistemic humility: the LLM ensemble provides a *consistent* and *replicable* measure of policy content, but whether it captures the governance quality that human experts would identify remains an open empirical question. The stability of findings across multiple robustness checks (see @sec-robustness) and the substantive interpretability of results (policies that score highly on the rubric are indeed those that practitioners and scholars recognise as operationally robust) provide reassuring face validity, but formal construct validation awaits the planned human coding study.
